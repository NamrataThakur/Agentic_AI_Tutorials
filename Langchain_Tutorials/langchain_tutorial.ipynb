{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "feb9f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser, XMLOutputParser\n",
    "from langchain.output_parsers import YamlOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, WebBaseLoader, ArxivLoader, WikipediaLoader\n",
    "#from langchain_unstructured import UnstructuredLoader\n",
    "import bs4\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, RecursiveJsonSplitter, HTMLHeaderTextSplitter, CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17bdb0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = os.getenv('LANGCHAIN_TRACING_V2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dde0150",
   "metadata": {},
   "source": [
    "##### Invoking the model for generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44958bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Namrata, it's nice to meet you!\n",
      "\n",
      "Is there anything I can help you with today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = ChatGroq(model=\"gemma2-9b-it\")\n",
    "print(model.invoke(\"My name is Namrata.\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1fc3c",
   "metadata": {},
   "source": [
    "##### Creating a chain with prompt and model. Invoking the chain for generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3be9236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Namrata!\n",
      "\n",
      "It's nice to meet you.  \n",
      "\n",
      "Is there anything specific I can help you with? Perhaps you have a question about AI, or maybe you'd like to explore some creative text generation? \n",
      "\n",
      "I'm here to assist in any way I can. 😊  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "    (\"user\",\"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model\n",
    "result = chain.invoke(input={'input':'My name is Namrata.'})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e11251e",
   "metadata": {},
   "source": [
    "#### Output Parsers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da2e299",
   "metadata": {},
   "source": [
    "##### Adding different parsers to the chain. Invoking the chain (with parsers) for generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b27d8b",
   "metadata": {},
   "source": [
    "StrOutputParser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a627ec17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is an open-source platform developed by **Replicate** that focuses on simplifying the process of building and deploying large language models (LLMs). \n",
      "\n",
      "Think of it as a user-friendly toolbox specifically designed for working with LLMs. Here's a breakdown of its key features and benefits:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **Streamlined Development:** Langsmith provides a visual interface and easy-to-use tools for fine-tuning and customizing pre-trained LLMs. You don't need to be a deep learning expert to get started.\n",
      "* **Experiment Tracking:** It offers robust tools for tracking experiments, comparing different model architectures and fine-tuning parameters, and managing your LLM development workflow efficiently.\n",
      "* **Collaboration:** Langsmith encourages collaboration by allowing teams to share models, datasets, and experiments, fostering a more open and collaborative LLM development ecosystem.\n",
      "* **Deployment & Scaling:**  It simplifies the process of deploying your fine-tuned LLMs to various platforms, including Replicate's cloud infrastructure, making it easy to share your creations with the world.\n",
      "* **Open-Source Nature:**  Being open-source means that Langsmith is accessible to everyone, encouraging community contributions and innovation.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Accessibility:**  Langsmith lowers the barrier to entry for LLM development, making it accessible to a wider range of users, including researchers, developers, and students.\n",
      "* **Efficiency:** Its streamlined tools and workflow help accelerate the LLM development process, allowing you to iterate faster and achieve your goals more efficiently.\n",
      "* **Transparency:**  The open-source nature of Langsmith promotes transparency and reproducibility in LLM research and development.\n",
      "\n",
      "**In essence, Langsmith is a powerful platform that empowers individuals and teams to harness the potential of LLMs without needing extensive technical expertise. It streamlines the development, deployment, and collaboration around these transformative models, fostering a more inclusive and innovative AI landscape.**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "str_op_parser = StrOutputParser()\n",
    "chain = prompt | model | str_op_parser\n",
    "result = chain.invoke(input={'input':'What is Langsmith?'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db17f151",
   "metadata": {},
   "source": [
    "JsonOutputParser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7a271ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      " \"definition\": \"Langsmith is an open-source framework for building and deploying AI agents.\",\n",
      " \"features\": [\n",
      "  \"Agent construction tools\",\n",
      "  \"Modular and extensible design\",\n",
      "  \"Integration with various AI models (LLMs, etc.)\",\n",
      "  \"Focus on task-oriented applications\"\n",
      " ],\n",
      " \"purpose\": \"To simplify the development and deployment of intelligent agents that can perform complex tasks.\"\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#OPTION 1: Provide the output format info in the system prompt itself.\n",
    "\n",
    "prompt_json = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question. Response should be in JSON object only.\"),\n",
    "    (\"user\",\"{input}\")\n",
    "])\n",
    "str_op_parser = StrOutputParser()\n",
    "chain = prompt_json | model | str_op_parser\n",
    "result = chain.invoke(input={'input':'What is Langsmith?'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "089daf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "  input_variables=['input'] input_types={} partial_variables={'format_instruction': 'Return a JSON object.'} template='You are an expert AI Engineer. Provide me answer based on the question. \\n {format_instruction} \\n {input}'\n",
      "\n",
      " {'name': 'Langsmith', 'description': 'Langsmith is an open-source platform for developing and deploying large language models (LLMs).', 'features': ['User-friendly interface for building and training LLMs', 'Pre-trained models and datasets for quick prototyping', 'Fine-tuning capabilities for customizing models to specific tasks', 'Model deployment options for integrating LLMs into applications', 'Community-driven development with active support and contributions'], 'benefits': ['Accelerated LLM development cycle', 'Reduced barrier to entry for LLM experimentation', 'Increased accessibility to powerful AI technologies', 'Collaborative environment for sharing and learning', 'Cost-effective solution for LLM development'], 'website': 'https://www.langsmith.ai/'}\n"
     ]
    }
   ],
   "source": [
    "#OPTION 2: Using the JsonOutputParser to decide the output format, instead of mentioning it explicitely in the system prompt. Using PromptTemplate\n",
    "\n",
    "json_op_parser = JsonOutputParser()\n",
    "output_format = json_op_parser.get_format_instructions()\n",
    "\n",
    "json_prompt = PromptTemplate(\n",
    "    template = \"You are an expert AI Engineer. Provide me answer based on the question. \\n {format_instruction} \\n {input}\",\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={'format_instruction':output_format}\n",
    "\n",
    ")\n",
    "print('Prompt:\\n ', json_prompt)\n",
    "\n",
    "chain = json_prompt | model | json_op_parser\n",
    "result = chain.invoke(input={'input':'What is Langsmith?'})\n",
    "print('\\n',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d159ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "  input_variables=['format_instruction', 'input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['format_instruction'], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question. \\n {format_instruction}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n",
      "{'name': 'Langsmith', 'description': 'Langsmith is an open-source platform for building and deploying AI applications.', 'features': ['User-friendly interface for creating and managing AI models', 'Support for multiple AI frameworks, including Llama 2', 'Built-in tools for fine-tuning and evaluating models', 'Deployment options for web, mobile, and cloud environments', 'Collaborative features for team development'], 'website': 'https://www.langsmith.com'}\n"
     ]
    }
   ],
   "source": [
    "#OPTION 3: Using the JsonOutputParser to decide the output format, instead of mentioning it explicitely in the system prompt. Using ChatPromptTemplate\n",
    "\n",
    "json_op_parser = JsonOutputParser()\n",
    "output_format = json_op_parser.get_format_instructions()\n",
    "\n",
    "prompt_json = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "            (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question. \\n {format_instruction}\"),\n",
    "            (\"user\",\"{input}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print('Prompt:\\n ', prompt_json)\n",
    "\n",
    "chain = prompt_json | model | json_op_parser\n",
    "result = chain.invoke(input={'input':'What is Langsmith?','format_instruction':{output_format}})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c11ab",
   "metadata": {},
   "source": [
    "Using Pydantic to standardize the LLM outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a220f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Alright,  let's talk about the latest and greatest iPhone!  You're likely talking about the **iPhone 15 series**, which was just released in September 2023. \n",
      "\n",
      "There are four models to choose from:\n",
      "\n",
      "* **iPhone 15:** This is the base model and it starts at **$799**.  It features a stunning 6.1-inch Super Retina XDR display, the powerful A16 Bionic chip (the same one as the iPhone 14 Pro), and a 48MP main camera. \n",
      "\n",
      "* **iPhone 15 Plus:**  For those who want a bigger screen, this model offers a 6.7-inch display and the same A16 Bionic chip as the iPhone 15. It starts at **$899**.\n",
      "\n",
      "* **iPhone 15 Pro:**  This is where things get really exciting!  The Pro model boasts a 6.1-inch display with a new ProMotion technology for even smoother scrolling and animations.  It's powered by the incredibly fast A17 Pro chip (built on a 3nm process, the most advanced chip in a smartphone) and has a new titanium frame for added durability.  It starts at **$999**.\n",
      "\n",
      "* **iPhone 15 Pro Max:**  The biggest and most powerful iPhone, the Pro Max features a 6.7-inch display, the A17 Pro chip, and a triple-lens camera system with a new periscope telephoto lens for incredible zoom capabilities.  It starts at **$1099**.\n",
      "\n",
      "**Here are some of the highlights of the iPhone 15 series:**\n",
      "\n",
      "* **USB-C Charging:**  All models now feature USB-C charging, a big change from the Lightning port used in previous iPhones.\n",
      "* **Dynamic Island:** This interactive notch at the top of the screen provides important information and quick access to apps.\n",
      "* **Improved Camera System:**  All models have upgraded cameras, with the Pro models featuring significant advancements like the periscope telephoto lens.\n",
      "* **A16 and A17 Pro Chips:**  The new A16 and A17 Pro chips deliver incredible performance and efficiency.\n",
      "\n",
      "I'm happy to answer any other questions you have about the iPhone 15 series. Which model are you most interested in learning more about?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using StrOutputParser:\n",
    "\n",
    "class Product(BaseModel):\n",
    "\n",
    "    product_name :str = Field(description=\"name of the product\")\n",
    "    price : int = Field(description=\"tentative price in USD\")\n",
    "\n",
    "str_parser = StrOutputParser(pydantic_object=Product)\n",
    "\n",
    "product_prompt = ChatPromptTemplate.from_messages([\n",
    "\n",
    "    (\"system\", \"You are expert Sales person who has vast knowledge on any product and it's description. You are well versed in product names and their prices.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "]\n",
    "\n",
    ")\n",
    "\n",
    "chain = product_prompt | model | str_parser\n",
    "result = chain.invoke(input={'input':'Tell me about the latest iPhone.'})\n",
    "print('\\n',result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb26d0a",
   "metadata": {},
   "source": [
    "Using just the pydantic object in the Parser is not enough. We need to provide the formating instructions too in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56589aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'product_name': 'iPhone 15 Pro Max', 'price': 1099}\n"
     ]
    }
   ],
   "source": [
    "#Using JsonOutputParser and Prompt Template:\n",
    "\n",
    "class Product(BaseModel):\n",
    "\n",
    "    product_name :str = Field(description=\"name of the product\")\n",
    "    price : int = Field(description=\"tentative price in USD\")\n",
    "\n",
    "str_parser = JsonOutputParser(pydantic_object=Product)\n",
    "output_format = str_parser.get_format_instructions()\n",
    "\n",
    "product_prompt = PromptTemplate(\n",
    "    template = \"You are expert sales person who has vast knowledge on any product and it's price. Provide an answer to the question. \\n {format_instruction} \\n {input}\",\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={'format_instruction':output_format}\n",
    "\n",
    ")\n",
    "\n",
    "chain = product_prompt | model | str_parser\n",
    "result = chain.invoke(input={'input':'Tell me about the latest iPhone.'})\n",
    "print('\\n',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05a41fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'product_name': 'iPhone 15 Pro Max', 'price': 1099}\n"
     ]
    }
   ],
   "source": [
    "#Using JsonOutputParser and ChatPrompt Template:\n",
    "\n",
    "class Product(BaseModel):\n",
    "\n",
    "    product_name :str = Field(description=\"name of the product\")\n",
    "    price : int = Field(description=\"tentative price in USD\")\n",
    "\n",
    "str_parser = JsonOutputParser(pydantic_object=Product)\n",
    "output_format = str_parser.get_format_instructions()\n",
    "\n",
    "product_prompt = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "            (\"system\",\"You are expert sales person who has vast knowledge on any product and it's price. Provide an answer to the question. \\n {format_instruction}\"),\n",
    "            (\"user\",\"{input}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "chain = product_prompt | model | str_parser\n",
    "result = chain.invoke(input={'input':'Tell me about the latest iPhone.', 'format_instruction':output_format})\n",
    "print('\\n',result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248ba8c6",
   "metadata": {},
   "source": [
    "Using Pydantic we can thus standardize the output of the LLMs. The output will always come using only the information required according to the pydantic class object passed to the parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bbca5b",
   "metadata": {},
   "source": [
    "Using Pydantic model on YamlOutputParser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed23655b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " product_name='iPhone 15 Pro Max' price=1099\n"
     ]
    }
   ],
   "source": [
    "#Using YamlOutputParser and Prompt Template:\n",
    "\n",
    "class Product(BaseModel):\n",
    "\n",
    "    product_name :str = Field(description=\"name of the product\")\n",
    "    price : int = Field(description=\"tentative price in USD\")\n",
    "\n",
    "str_parser = YamlOutputParser(pydantic_object=Product)\n",
    "output_format = str_parser.get_format_instructions()\n",
    "\n",
    "product_prompt = PromptTemplate(\n",
    "    template = \"You are expert sales person who has vast knowledge on any product and it's price. Provide an answer to the question. \\n {format_instruction} \\n {input}\",\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={'format_instruction':output_format}\n",
    "\n",
    ")\n",
    "\n",
    "chain = product_prompt | model | str_parser\n",
    "result = chain.invoke(input={'input':'Tell me about the latest iPhone.'})\n",
    "print('\\n',result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0004c165",
   "metadata": {},
   "source": [
    "#### Data Ingestion Techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777119b3",
   "metadata": {},
   "source": [
    "Langchain Documentation: https://python.langchain.com/docs/integrations/document_loaders/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6506002",
   "metadata": {},
   "source": [
    "TextLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b561702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]\n",
      "<class 'langchain_core.documents.base.Document'>\n",
      "Metadata:  {'source': 'speech.txt'}\n",
      "Total Docs:  1\n"
     ]
    }
   ],
   "source": [
    "text_loader = TextLoader('speech.txt')\n",
    "text_docs= text_loader.load() # Load data into Document Objects\n",
    "print(text_docs)\n",
    "print(type(text_docs[0]))\n",
    "print('Metadata: ', text_docs[0].metadata)\n",
    "print('Total Docs: ', len(text_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c5b9c",
   "metadata": {},
   "source": [
    "PdPDFLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "899e6308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n"
     ]
    }
   ],
   "source": [
    "pdf_loader = PyPDFLoader('foundation_of_llm_tao_zhu.pdf')\n",
    "pdf_docs = pdf_loader.load()\n",
    "print(len(pdf_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd05facc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='4 Pre-training\n",
      "Unsupervised Supervised\n",
      "Pre-training Training\n",
      "Unlabeled\n",
      "Data\n",
      "Labeled\n",
      "Data\n",
      "(a) Unsupervised Pre-training\n",
      "Supervised Supervised\n",
      "Pre-training Tuning\n",
      "Labeled\n",
      "Data\n",
      "T ask 1\n",
      "Labeled\n",
      "Data\n",
      "T ask 2\n",
      "(b) Supervised Pre-training\n",
      "Self-\n",
      "Supervised\n",
      "Supervised\n",
      "Zero/Few\n",
      "Shot Learning\n",
      "Pre-training Tuning\n",
      "Prompting\n",
      "Unlabeled\n",
      "Data\n",
      "Labeled\n",
      "Data\n",
      "(c) Self-supervised Pre-training\n",
      "Fig. 1.1: Illustration of unsupervised, supervised, and self-super vised pre-training. In unsupervised pre-training, the\n",
      "pre-training is performed on large-scale unlabeled data. I t can be viewed as a preliminary step to have a good starting\n",
      "point for the subsequent optimization process, though cons iderable effort is still required to further train the model\n",
      "with labeled data after pre-training. In supervised pre-tr aining, the underlying assumption is that different (super vised)\n",
      "learning tasks are related. So we can ﬁrst train the model on o ne task, and transfer the resulting model to another task\n",
      "with some training or tuning effort. In self-supervised pre -training, a model is pre-trained on large-scale unlabeled data\n",
      "via self-supervision. The model can be well trained in this w ay , and we can efﬁciently adapt it to new tasks through\n",
      "ﬁne-tuning or prompting.\n",
      "• Sequence Generation Models . In NLP , sequence generation generally refers to the prob-\n",
      "lem of generating a sequence of tokens based on a given contex t. The term context has\n",
      "different meanings across applications. For example, it re fers to the preceding tokens in\n",
      "language modeling, and refers to the source-language seque nce in machine translation\n",
      "2 .\n",
      "W e need different techniques for applying these models to do wnstream tasks after pre-training.\n",
      "Here we are interested in the following two methods.\n",
      "1.1.2.1 Fine-tuning of Pre-trained Models\n",
      "For sequence encoding pre-training, a common method of adap ting pre-trained models is ﬁne-\n",
      "tuning. Let Encodeθ (·) denote an encoder with parameters θ, for example, Encodeθ (·) can be a\n",
      "standard Transformer encoder. Provided we have pre-traine d this model in some way and obtained\n",
      "the optimal parameters ˆθ, we can employ it to model any sequence and generate the corre sponding\n",
      "representation, like this\n",
      "H = Encode ˆθ (x) (1.2)\n",
      "where x is the input sequence {x0 ,x1 ,...,xm }, and H is the output representation which is a\n",
      "sequence of real-valued vectors {h0 ,h1,...,hm }. Because the encoder does not work as a stan-\n",
      "dalone NLP system, it is often integrated as a component into a bigger system. Consider, for\n",
      "example, a text classiﬁcation problem in which we identify t he polarity (i.e., positive, negative,\n",
      "2 More precisely , in auto-regressive decoding of machine tra nslation, each target-language token is generated based\n",
      "on both its preceding tokens and source-language sequence.' metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'foundation_of_llm_tao_zhu.pdf', 'total_pages': 231, 'page': 10, 'page_label': '4'}\n"
     ]
    }
   ],
   "source": [
    "print(pdf_docs[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21291373",
   "metadata": {},
   "source": [
    "UnstructedLoader:\n",
    "Need to research more how to use it in local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81614abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uns_loader = UnstructuredLoader('foundation_of_llm_tao_zhu.pdf')\n",
    "# pdf_uns_docs = uns_loader.load()\n",
    "# print(len(pdf_uns_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11df4c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pdf_uns_docs[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86acab22",
   "metadata": {},
   "source": [
    "WebLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d021296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "web_loader = WebBaseLoader(web_path=\"https://www.resumewriter.sg/blog/list-of-headhunters-in-singapore/\",\n",
    "                           bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                           class_=(\"post-title\",\"post-content\",\"post-header\") #See the video to understand this:\n",
    "                        ))\n",
    "                        )\n",
    "\n",
    "web_docs = web_loader.load()\n",
    "print(len(web_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c30d3578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='' metadata={'source': 'https://www.resumewriter.sg/blog/list-of-headhunters-in-singapore/'}\n"
     ]
    }
   ],
   "source": [
    "print(web_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6ee1e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "web_loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "                     ))\n",
    "                     )\n",
    "web_docs = web_loader.load()\n",
    "print(len(web_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9416d5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "\n",
      "Tool use\n",
      "\n",
      "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overview of a LLM-powered autonomous agent system.\n",
      "\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\n",
      "ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\n",
      "The ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\n",
      "Thought: ...\n",
      "Action: ...\n",
      "Observation: ...\n",
      "... (Repeated many times)\n",
      "\n",
      "\n",
      "Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\n",
      "\n",
      "In both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\n",
      "Reflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\n",
      "\n",
      "\n",
      "Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "\n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\n",
      "\n",
      "\n",
      "Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\n",
      "\n",
      "Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\n",
      "To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\n",
      "The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\n",
      "\n",
      "\n",
      "After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\n",
      "\n",
      "The idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\n",
      "\n",
      "\n",
      "Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\n",
      "\n",
      "The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\n",
      "In reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\n",
      "In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\n",
      "\n",
      "\n",
      "Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\n",
      "\n",
      "Component Two: Memory#\n",
      "(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\n",
      "Types of Memory#\n",
      "Memory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\n",
      "\n",
      "\n",
      "Sensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\n",
      "\n",
      "\n",
      "Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\n",
      "\n",
      "\n",
      "Long-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\n",
      "\n",
      "Explicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\n",
      "Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Categorization of human memory.\n",
      "\n",
      "We can roughly consider the following mappings:\n",
      "\n",
      "Sensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\n",
      "Short-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\n",
      "Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\n",
      "\n",
      "Maximum Inner Product Search (MIPS)#\n",
      "The external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)​ algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\n",
      "A couple common choices of ANN algorithms for fast MIPS:\n",
      "\n",
      "LSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\n",
      "ANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\n",
      "HNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\n",
      "FAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\n",
      "ScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\tilde{x}_i$ such that the inner product $\\langle q, x_i \\rangle$ is as similar to the original distance of $\\angle q, \\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\n",
      "\n",
      "\n",
      "\n",
      "Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\n",
      "\n",
      "Check more MIPS algorithms and performance comparison in ann-benchmarks.com.\n",
      "Component Three: Tool Use#\n",
      "Tool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\n",
      "\n",
      "\n",
      "A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\n",
      "\n",
      "MRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\n",
      "They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\n",
      "Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\n",
      "ChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\n",
      "HuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\n",
      "\n",
      "\n",
      "Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\n",
      "\n",
      "The system comprises of 4 stages:\n",
      "(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\n",
      "Instruction:\n",
      "\n",
      "The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\n",
      "\n",
      "(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\n",
      "Instruction:\n",
      "\n",
      "Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\n",
      "\n",
      "(3) Task execution: Expert models execute on the specific tasks and log results.\n",
      "Instruction:\n",
      "\n",
      "With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\n",
      "\n",
      "(4) Response generation: LLM receives the execution results and provides summarized results to users.\n",
      "To put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\n",
      "API-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\n",
      "\n",
      "\n",
      "Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\n",
      "\n",
      "In the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\n",
      "\n",
      "Whether an API call is needed.\n",
      "Identify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\n",
      "Response based on the API results: the model can choose to refine and call again if results are not satisfied.\n",
      "\n",
      "This benchmark evaluates the agent’s tool use capabilities at three levels:\n",
      "\n",
      "Level-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\n",
      "Level-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\n",
      "Level-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\n",
      "\n",
      "Case Studies#\n",
      "Scientific Discovery Agent#\n",
      "ChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\n",
      "\n",
      "The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\n",
      "It is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\n",
      "\n",
      "One interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\n",
      "Boiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\n",
      "For example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\n",
      "\n",
      "inquired about current trends in anticancer drug discovery;\n",
      "selected a target;\n",
      "requested a scaffold targeting these compounds;\n",
      "Once the compound was identified, the model attempted its synthesis.\n",
      "\n",
      "They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\n",
      "Generative Agents Simulation#\n",
      "Generative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\n",
      "The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\n",
      "\n",
      "Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\n",
      "\n",
      "Each element is an observation, an event directly provided by the agent.\n",
      "- Inter-agent communication can trigger new natural language statements.\n",
      "\n",
      "\n",
      "Retrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\n",
      "\n",
      "Recency: recent events have higher scores\n",
      "Importance: distinguish mundane from core memories. Ask LM directly.\n",
      "Relevance: based on how related it is to the current situation / query.\n",
      "\n",
      "\n",
      "Reflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\n",
      "\n",
      "Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\n",
      "\n",
      "\n",
      "Planning & Reacting: translate the reflections and the environment information into actions\n",
      "\n",
      "Planning is essentially in order to optimize believability at the moment vs in time.\n",
      "Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\n",
      "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
      "Environment information is present in a tree structure.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The generative agent architecture. (Image source: Park et al. 2023)\n",
      "\n",
      "This fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\n",
      "Proof-of-Concept Examples#\n",
      "AutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\n",
      "Here is the system message used by AutoGPT, where {{...}} are user inputs:\n",
      "You are {{ai-name}}, {{user-provided AI bot description}}.\n",
      "Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
      "\n",
      "GOALS:\n",
      "\n",
      "1. {{user-provided goal 1}}\n",
      "2. {{user-provided goal 2}}\n",
      "3. ...\n",
      "4. ...\n",
      "5. ...\n",
      "\n",
      "Constraints:\n",
      "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
      "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
      "3. No user assistance\n",
      "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
      "5. Use subprocesses for commands that will not terminate within a few minutes\n",
      "\n",
      "Commands:\n",
      "1. Google Search: \"google\", args: \"input\": \"<search>\"\n",
      "2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\n",
      "3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\n",
      "4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\n",
      "5. List GPT Agents: \"list_agents\", args:\n",
      "6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\n",
      "7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\n",
      "8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n",
      "9. Read file: \"read_file\", args: \"file\": \"<file>\"\n",
      "10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n",
      "11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\n",
      "12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\n",
      "13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\n",
      "14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\n",
      "15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\n",
      "16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\n",
      "17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\n",
      "18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\n",
      "19. Do Nothing: \"do_nothing\", args:\n",
      "20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\n",
      "\n",
      "Resources:\n",
      "1. Internet access for searches and information gathering.\n",
      "2. Long Term memory management.\n",
      "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
      "4. File output.\n",
      "\n",
      "Performance Evaluation:\n",
      "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
      "2. Constructively self-criticize your big-picture behavior constantly.\n",
      "3. Reflect on past decisions and strategies to refine your approach.\n",
      "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
      "\n",
      "You should only respond in JSON format as described below\n",
      "Response Format:\n",
      "{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"thought\",\n",
      "        \"reasoning\": \"reasoning\",\n",
      "        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n",
      "        \"criticism\": \"constructive self-criticism\",\n",
      "        \"speak\": \"thoughts summary to say to user\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"command name\",\n",
      "        \"args\": {\n",
      "            \"arg name\": \"value\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Ensure the response can be parsed by Python json.loads\n",
      "GPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\n",
      "Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\nThen you will pick one clarifying question, and wait for an answer from the user.\\n\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\n\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Summary of areas that need clarification:\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\n2. Details about the MVC components (e.g. which components are in each file)\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\n\\nClarifying question:\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\n\\nIs anything else unclear? If yes, only answer in the form:\\n{remaining unclear areas} remaining questions.\\n{Next question}\\nIf everything is sufficiently clear, only answer \\\"Nothing more to clarify.\\\".\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Remaining unclear areas: 2 remaining questions.\\nCan you provide more information about how the MVC components are split into separate files?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\n",
      "  }\n",
      "]\n",
      "Then after these clarification, the agent moved into the code writing mode with a different system message.\n",
      "System message:\n",
      "\n",
      "You will get instructions for code to write.\n",
      "You will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\n",
      "Make sure that every detail of the architecture is, in the end, implemented as code.\n",
      "Think step by step and reason yourself to the right decisions to make sure we get it right.\n",
      "You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\n",
      "Then you will output the content of each file including ALL code.\n",
      "Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that\n",
      "FILENAME is the lowercase file name including the file extension,\n",
      "LANG is the markup code block language for the code’s language, and CODE is the code:\n",
      "FILENAME\n",
      "CODE\n",
      "You will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\n",
      "Please note that the code should be fully functional. No placeholders.\n",
      "Follow a language and framework appropriate best practice file naming convention.\n",
      "Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\n",
      "Ensure to implement all code, if you are unsure, write a plausible implementation.\n",
      "Include module dependency or package manager dependency definition file.\n",
      "Before you finish, double check that all parts of the architecture is present in the files.\n",
      "Useful to know:\n",
      "You almost always put different classes in different files.\n",
      "For Python, you always create an appropriate requirements.txt file.\n",
      "For NodeJS, you always create an appropriate package.json file.\n",
      "You always add a comment briefly describing the purpose of the function definition.\n",
      "You try to add comments explaining very complex bits of logic.\n",
      "You always follow the best practices for the requested languages in terms of describing the code written as a defined\n",
      "package/project.\n",
      "Python toolbelt preferences:\n",
      "\n",
      "pytest\n",
      "dataclasses\n",
      "\n",
      "\n",
      "Conversatin samples:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\n\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\n\\n\\nPython toolbelt preferences:\\n- pytest\\n- dataclasses\\n\"\n",
      "  },\n",
      " #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Assumptions:\\n1. Model: The model will contain the game's data, such as level information, character states, and enemy positions.\\n2. View: The view will handle the game's visuals, including rendering the game objects, backgrounds, and updating the display.\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\n\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Please now remember the steps:\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nPlease note that the code should be fully functional. No placeholders.\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\"\n",
      "  }\n",
      "]\n",
      "Challenges#\n",
      "After going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\n",
      "\n",
      "\n",
      "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n",
      "\n",
      "\n",
      "Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\n",
      "\n",
      "\n",
      "Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\n",
      "\n",
      "\n",
      "Citation#\n",
      "Cited as:\n",
      "\n",
      "Weng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\n",
      "\n",
      "Or\n",
      "@article{weng2023agent,\n",
      "  title   = \"LLM-powered Autonomous Agents\",\n",
      "  author  = \"Weng, Lilian\",\n",
      "  journal = \"lilianweng.github.io\",\n",
      "  year    = \"2023\",\n",
      "  month   = \"Jun\",\n",
      "  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
      "}\n",
      "References#\n",
      "[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\n",
      "[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\n",
      "[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\n",
      "“ arXiv preprint arXiv:2302.02676 (2023).\n",
      "[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\n",
      "[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\n",
      "[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\n",
      "[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n",
      "[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\n",
      "[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\n",
      "[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\n",
      "[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\n",
      "[12] Parisi et al. “TALM: Tool Augmented Language Models”\n",
      "[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\n",
      "[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\n",
      "[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\n",
      "[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\n",
      "[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\n",
      "[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\n",
      "[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\n",
      "[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\n",
      "[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\n",
      "' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n"
     ]
    }
   ],
   "source": [
    "print(web_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03a3936",
   "metadata": {},
   "source": [
    "ArxivLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1b4eab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "arx_loader = ArxivLoader(query=\"2007.14390\",doc_content_chars_max=10000000000)\n",
    "arx_docs = arx_loader.load()\n",
    "print(len(arx_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0d7b6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='FLOWER: A FRIENDLY FEDERATED LEARNING FRAMEWORK\n",
      "Daniel J. Beutel 1 2 Taner Topal 1 2 Akhil Mathur 3 Xinchi Qiu 1 Javier Fernandez-Marques 4 Yan Gao 1\n",
      "Lorenzo Sani 5 Kwing Hei Li 1 Titouan Parcollet 6 Pedro Porto Buarque de Gusm˜ao 1 Nicholas D. Lane 1\n",
      "ABSTRACT\n",
      "Federated Learning (FL) has emerged as a promising technique for edge devices to collaboratively learn a shared\n",
      "prediction model, while keeping their training data on the device, thereby decoupling the ability to do machine\n",
      "learning from the need to store the data in the cloud. However, FL is difﬁcult to implement realistically, both\n",
      "in terms of scale and systems heterogeneity. Although there are a number of research frameworks available to\n",
      "simulate FL algorithms, they do not support the study of scalable FL workloads on heterogeneous edge devices.\n",
      "In this paper, we present Flower – a comprehensive FL framework that distinguishes itself from existing platforms\n",
      "by offering new facilities to execute large-scale FL experiments, and consider richly heterogeneous FL device\n",
      "scenarios. Our experiments show Flower can perform FL experiments up to 15M in client size using only a pair of\n",
      "high-end GPUs. Researchers can then seamlessly migrate experiments to real devices to examine other parts of\n",
      "the design space. We believe Flower provides the community a critical new tool for FL study and development.\n",
      "1\n",
      "INTRODUCTION\n",
      "There has been tremendous progress in enabling the exe-\n",
      "cution of deep learning models on mobile and embedded\n",
      "devices to infer user contexts and behaviors (Fromm et al.,\n",
      "2018; Chowdhery et al., 2019; Malekzadeh et al., 2019;\n",
      "Lee et al., 2019; Yao et al., 2019; LiKamWa et al., 2016;\n",
      "Georgiev et al., 2017). This has been powered by the in-\n",
      "creasing computational abilities of mobile devices as well\n",
      "as novel algorithms which apply software optimizations to\n",
      "enable pre-trained cloud-scale models to run on resource-\n",
      "constrained devices. However, when it comes to the training\n",
      "of these mobile-focused models, a working assumption has\n",
      "been that the models will be trained centrally in the cloud,\n",
      "using training data aggregated from several users.\n",
      "Federated Learning (FL) (McMahan et al., 2017) is an\n",
      "emerging area of research in the machine learning com-\n",
      "munity which aims to enable distributed edge devices (or\n",
      "users) to collaboratively train a shared prediction model\n",
      "while keeping their personal data private. At a high level,\n",
      "this is achieved by repeating three basic steps: i) local pa-\n",
      "rameters update to a shared prediction model on each edge\n",
      "device, ii) sending the local parameter updates to a central\n",
      "server for aggregation, and iii) receiving the aggregated\n",
      "1Department of Computer Science and Technology, University\n",
      "of Cambridge, UK 2Adap, Hamburg, Hamburg, Germany 3Nokia\n",
      "Bell Labs, Cambridge, UK 4Department of Computer Science,\n",
      "University of Oxford, UK 5Department of Physics and Astronomy,\n",
      "University of Bologna, Italy 6Laboratoire Informatique d’Avignon,\n",
      "Avignon Universit´e, France. Correspondence to: Daniel J. Beutel\n",
      "<daniel@adap.com>.\n",
      "10\n",
      "1\n",
      "10\n",
      "2\n",
      "10\n",
      "3\n",
      "10\n",
      "4\n",
      "10\n",
      "5\n",
      "10\n",
      "6\n",
      "Total # of clients in the pool\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "Concurrent # clients\n",
      "Others\n",
      "FedScale\n",
      "Flower\n",
      "Figure 1. Survey of the number of FL clients used in FL research\n",
      "papers in the last two years. Scatter plot of number of concurrent\n",
      "clients participated in each communication round (y-axis) and total\n",
      "number of clients in the client pool (x-axis). The x-axis is con-\n",
      "verted to log scale to reﬂect the data points more clearly. FedScale\n",
      "can achieve 100 concurrent clients participated in each round out\n",
      "of 10000 total clients (orange point), while Flower framework can\n",
      "achieve 1000 concurrent clients out of a total 1 million clients\n",
      "(green point). The plot shows that Flower can achieve both higher\n",
      "concurrent participated client and larger client pool compared with\n",
      "other experiments existing the the recent research papers. Ap-\n",
      "pendix A.1 gives details of the papers considered.\n",
      "model back for the next round of local updates.\n",
      "From a systems perspective, a major bottleneck to FL\n",
      "research is the paucity of frameworks that support scal-\n",
      "able execution of FL methods on mobile and edge de-\n",
      "vices.\n",
      "While several frameworks including Tensorﬂow\n",
      "Federated (Google, 2020; Abadi et al., 2016a) (TFF) and\n",
      "LEAF (Caldas et al., 2018) enable experimentation on FL\n",
      "algorithms, they do not provide support for running FL on\n",
      "arXiv:2007.14390v5  [cs.LG]  5 Mar 2022\n",
      "Flower: A Friendly Federated Learning Framework\n",
      "edge devices. System-related factors such as heterogeneity\n",
      "in the software stack, compute capabilities, and network\n",
      "bandwidth, affect model synchronization and local training.\n",
      "In combination with the choice of the client selection and\n",
      "parameter aggregation algorithms, they can impact the ac-\n",
      "curacy and training time of models trained in a federated\n",
      "setting. The systems’ complexity of FL and the lack of\n",
      "scalable open-source frameworks can lead to a disparity be-\n",
      "tween FL research and production. While closed production-\n",
      "grade systems report client numbers in the thousands or even\n",
      "millions (Hard et al., 2019), few research papers use popu-\n",
      "lations of more than 100 clients, as can be seen in Figure 1.\n",
      "Even those papers which use more than 100 clients rely on\n",
      "simulations (e.g., using nested loops) rather than actually\n",
      "implementing FL clients on real devices.\n",
      "In this paper, we present Flower1, a novel FL framework,\n",
      "that supports experimentation with both algorithmic and\n",
      "systems-related challenges in FL. Flower offers a stable, lan-\n",
      "guage and ML framework-agnostic implementation of the\n",
      "core components of a FL system, and provides higher-level\n",
      "abstractions to enable researchers to experiment and imple-\n",
      "ment new ideas on top of a reliable stack. Moreover, Flower\n",
      "allows for rapid transition of existing ML training pipelines\n",
      "into a FL setup to evaluate their convergence properties\n",
      "and training time in a federated setting. Most importantly,\n",
      "Flower provides support for extending FL implementations\n",
      "to mobile and wireless clients, with heterogeneous compute,\n",
      "memory, and network resources.\n",
      "As system-level challenges of limited compute, memory,\n",
      "and network bandwidth in mobile devices are not a major\n",
      "bottleneck for powerful cloud servers, Flower provides built-\n",
      "in tools to simulate many of these challenging conditions in\n",
      "a cloud environment and allows for a realistic evaluation of\n",
      "FL algorithms. Finally, Flower is designed with scalability\n",
      "in mind and enables large-cohort research that leverages\n",
      "both a large number of connected clients and a large num-\n",
      "ber of clients training concurrently. We believe that the\n",
      "capability to perform FL at scale will unlock new research\n",
      "opportunities as results obtained in small-scale experiments\n",
      "are not guaranteed to generalize well to large-scale problems.\n",
      "In summary, we make the following contributions:\n",
      "• We present Flower, a novel FL framework that supports\n",
      "large-cohort training and evaluation, both on real edge\n",
      "devices and on single-node or multi-node compute clus-\n",
      "ters. This unlocks scalable algorithmic research of real-\n",
      "world system conditions such as limited computational\n",
      "resources which are common for typical FL workloads.\n",
      "• We describe the design principles and implementation\n",
      "details of Flower. In addition to being language- and\n",
      "ML framework-agnostic by design, Flower is also fully\n",
      "1https://flower.dev\n",
      "extendable and can incorporate emerging algorithms,\n",
      "training strategies and communication protocols.\n",
      "• Using Flower , we present experiments that explore both\n",
      "algorithmic and system-level aspects of FL on ﬁve ma-\n",
      "chine learning workloads with up to 15 million clients.\n",
      "Our results quantify the impact of various system bot-\n",
      "tlenecks such as client heterogeneity and ﬂuctuating\n",
      "network speeds on FL performance.\n",
      "• Flower is open-sourced under Apache 2.0 License\n",
      "and adopted by major research organizations in both\n",
      "academia and industry. The community is actively par-\n",
      "ticipating in the development and contributes novel base-\n",
      "lines, functionality, and algorithms.\n",
      "2\n",
      "BACKGROUND AND RELATED WORK\n",
      "FL builds on a vast body of prior work and has since been\n",
      "expanded in different directions. McMahan et al. (2017)\n",
      "introduced the basic federated averaging (FedAvg) algo-\n",
      "rithm and evaluated it in terms of communication efﬁciency.\n",
      "There is active work on privacy and robustness improve-\n",
      "ments for FL: A targeted model poisoning attack using\n",
      "Fashion-MNIST (Xiao et al., 2017) (along with possible mit-\n",
      "igation strategies) was demonstrated by Bhagoji et al. (2018).\n",
      "Abadi et al. (2016b) propose an attempt to translate the idea\n",
      "of differential privacy to deep learning. Secure aggrega-\n",
      "tion (Bonawitz et al., 2017) is a way to hide model updates\n",
      "from “honest but curious” attackers. Robustness and fault-\n",
      "tolerance improvements at the optimizer level are commonly\n",
      "studied and demonstrated, e.g., by Zeno (Xie et al., 2019).\n",
      "Finally, there is an increasing emphasis on the performance\n",
      "of federated optimization in heterogeneous data and system\n",
      "settings (Smith et al., 2017; Li et al., 2018; 2019).\n",
      "The optimization of distributed training with and without\n",
      "federated concepts has been covered from many angles\n",
      "(Dean et al., 2012; Jia et al., 2018; Chahal et al., 2018;\n",
      "Sergeev & Balso, 2018; Dryden et al., 2016). Bonawitz et\n",
      "al. (2019) detail the system design of a large-scale Google-\n",
      "internal FL system. TFF (Google, 2020), PySyft (Ryffel\n",
      "et al., 2018), and LEAF (Caldas et al., 2018) propose open\n",
      "source frameworks which are primarily used for simulations\n",
      "that run a small number of homogeneous clients. Flower\n",
      "uniﬁes both perspectives by being open source and suitable\n",
      "for exploratory research, with scalability to expand into\n",
      "settings involving a large number of heterogeneous clients.\n",
      "Most of the mentioned approaches have in common that they\n",
      "implement their own systems to obtain the described results.\n",
      "The main intention of Flower is to provide a framework\n",
      "which would (a) allow to perform similar research using a\n",
      "common framework and (b) enable to run those experiments\n",
      "on a large number of heterogeneous devices.\n",
      "Flower: A Friendly Federated Learning Framework\n",
      "3\n",
      "FLOWER OVERVIEW\n",
      "Flower is a novel end-to-end federated learning framework\n",
      "that enables a more seamless transition from experimental\n",
      "research in simulation to system research on a large cohort\n",
      "of real edge devices. Flower offers individual strength in\n",
      "both areas (viz. simulation and real world devices); and of-\n",
      "fers the ability for experimental implementations to migrate\n",
      "between the two extremes as needed during exploration and\n",
      "development. In this section, we describe use cases that\n",
      "motivate our perspective, design goals, resulting framework\n",
      "architecture, and comparison to other frameworks.\n",
      "3.1\n",
      "Use Cases\n",
      "The identiﬁed gap between FL research practice and indus-\n",
      "try reports from proprietary large-scale systems (Figure 1)\n",
      "is, at least in part, related a number of use cases that are not\n",
      "well-supported by the current FL ecosystem. The following\n",
      "sections show how Flower enables those use cases.\n",
      "Scale experiments to large cohorts. Experiments need to\n",
      "scale to both a large client pool size and a large number of\n",
      "clients training concurrently to better understand how well\n",
      "methods generalize. A researcher needs to be able launch\n",
      "large-scale FL evaluations of their algorithms and design\n",
      "using reasonable levels of compute (e.g., single-machine/a\n",
      "multi-GPU rack), and have results at this scale have accept-\n",
      "able speed (wall-clock execution time).\n",
      "Experiment on heterogeneous devices. Heterogeneous\n",
      "client environments are the norm for FL. Researchers need\n",
      "ways to both simulate heterogeneity and to execute FL on\n",
      "real edge devices to quantify the effects of system hetero-\n",
      "geneity. Measurements about the performance of client\n",
      "performance should be able to be easily collected, and de-\n",
      "ploying heterogeneous experiments is painless.\n",
      "Transition from simulation to real devices. New methods\n",
      "are often conceived in simulated environments. To under-\n",
      "stand their applicability to real-world scenarios, frameworks\n",
      "need to support seamless transition between simulation and\n",
      "on-device execution. Shifting from simulation to real de-\n",
      "vices, mixing simulated and real devices, and selecting cer-\n",
      "tain elements to have varying levels of realism (e.g., com-\n",
      "pute or network) should be easy.\n",
      "Multi-framework workloads.\n",
      "Diverse client environ-\n",
      "ments naturally motivate the usage of different ML frame-\n",
      "works, so FL frameworks should be able to integrate updates\n",
      "coming from clients using varying ML frameworks in the\n",
      "same workload. Examples range from situations where\n",
      "clients use two different training frameworks (pytorch and\n",
      "tensorﬂow) to more complex situations where clients have\n",
      "their own device- and OS-speciﬁc training algorithm.\n",
      "Table 1. Excerpt of built-in FL algorithms available in Flower. New\n",
      "algorithms can be implemented using the Strategy interface.\n",
      "Strategy\n",
      "Description\n",
      "FedAvg\n",
      "Vanilla Federated Averaging (McMahan et al., 2017)\n",
      "Fault\n",
      "Tolerant\n",
      "FedAvg\n",
      "A variant of FedAvg that can tolerate faulty client\n",
      "conditions such as client disconnections or laggards.\n",
      "FedProx\n",
      "Implementation of the algorithm proposed by\n",
      "Li et al. (2020) to extend FL to heterogenous\n",
      "network conditions.\n",
      "QFedAvg\n",
      "Implementation of the algorithm proposed by\n",
      "Li et al. (2019) to encourage fairness in FL.\n",
      "FedOptim\n",
      "A family of server-side optimizations that\n",
      "include FedAdagrad, FedYogi, and FedAdam\n",
      "as described in Reddi et al. (2021).\n",
      "3.2\n",
      "Design Goals\n",
      "The given uses cases identify a gap in the existing FL ecosys-\n",
      "tem that results in research that does not necessarily reﬂect\n",
      "real-world FL scenarios. To adress the ecosystem gap, we\n",
      "deﬁned a set of independent design goals for Flower:\n",
      "Scalable: Given that real-world FL would encounter a large\n",
      "number of clients, Flower should scale to a large number of\n",
      "concurrent clients to foster research on a realistic scale.\n",
      "Client-agnostic: Given the heterogeneous environment on\n",
      "mobile clients, Flower should be interoperable with different\n",
      "programming languages, operating systems, and hardware.\n",
      "Communication-agnostic: Given the heterogeneous con-\n",
      "nectivity settings, Flower should allow different serialization\n",
      "and communication approaches.\n",
      "Privacy-agnostic: Different FL settings (cross-devic, cross-\n",
      "silo) have different privacy requirements (secure aggrega-\n",
      "tion, differential privacy). Flower should support common\n",
      "approaches whilst not be prescriptive about their usage.\n",
      "Flexible: Given the rate of change in FL and the velocity\n",
      "of the general ML ecosystem, Flower should be ﬂexible to\n",
      "enable both experimental research and adoption of recently\n",
      "proposed approaches with low engineering overhead.\n",
      "A framework architecture with those properties will increase\n",
      "both realism and scale in FL research and provide a smooth\n",
      "transition from research in simulation to large-cohort re-\n",
      "search on real edge devices. The next section describes how\n",
      "the Flower framework architecture supports those goals.\n",
      "3.3\n",
      "Core Framework Architecture\n",
      "FL can be described as an interplay between global and\n",
      "local computations. Global computations are executed on\n",
      "the server side and responsible for orchestrating the learning\n",
      "Flower: A Friendly Federated Learning Framework\n",
      "Figure 2. Flower core framework architecture with both Edge\n",
      "Client Engine and Virtual Client Engine. Edge clients live on\n",
      "real edge devices and communicate with the server over RPC.\n",
      "Virtual clients on the other hand consume close to zero resources\n",
      "when inactive and only load model and data into memory when\n",
      "the client is being selected for training or evaluation.\n",
      "process over a set of available clients. Local computations\n",
      "are executed on individual clients and have access to actual\n",
      "data used for training or evaluation of model parameters.\n",
      "The architecture of the Flower core framework reﬂects that\n",
      "perspective and enables researchers to experiment with\n",
      "building blocks, both on the global and on the local level.\n",
      "Global logic for client selection, conﬁguration, parameter\n",
      "update aggregation, and federated or centralized model eval-\n",
      "uation can be expressed through the Strategy abstraction.\n",
      "An implementation of the Strategy abstraction represents\n",
      "a single FL algorithm and Flower provides tested refer-\n",
      "ence implementations of popular FL algorithms such as\n",
      "FedAvg (McMahan et al., 2017) or FedYogi (Reddi et al.,\n",
      "2021) (summarized in table 1). Local logic on the other\n",
      "hand is mainly concerned with model training and evalu-\n",
      "ation on local data partitions. Flower acknowledges the\n",
      "breadth and diversity of existing ML pipelines and offers\n",
      "ML framework-agnostic ways to federate these, either on\n",
      "the Flower Protocol level or using the high-level Client\n",
      "abstraction. Figure 2 illustrates those components.\n",
      "The Flower core framework implements the necessary in-\n",
      "frastructure to run these workloads at scale. On the server\n",
      "side, there are three major components involved: the Client-\n",
      "Manager, the FL loop, and a (user customizable) Strategy.\n",
      "Server components sample clients from the ClientManager,\n",
      "which manages a set of ClientProxy objects, each repre-\n",
      "senting a single client connected to the server. They are\n",
      "responsible for sending and receiving Flower Protocol mes-\n",
      "sages to and from the actual client. The FL loop is at the\n",
      "heart of the FL process: it orchestrates the entire learning\n",
      "process. It does not, however, make decisions about how\n",
      "to proceed, as those decisions are delegated to the currently\n",
      "conﬁgured Strategy implementation.\n",
      "In summary, the FL loop asks the Strategy to conﬁgure the\n",
      "next round of FL, sends those conﬁgurations to the affected\n",
      "clients, receives the resulting client updates (or failures)\n",
      "from the clients, and delegates result aggregation to the\n",
      "Strategy. It takes the same approach for both federated\n",
      "training and federated evaluation, with the added capability\n",
      "of server-side evaluation (again, via the Strategy). The client\n",
      "side is simpler in the sense that it only waits for messages\n",
      "from the server. It then reacts to the messages received by\n",
      "calling user-provided training and evaluation functions.\n",
      "A distinctive property of this architecture is that the server\n",
      "is unaware of the nature of connected clients, which al-\n",
      "lows to train models across heterogeneous client platforms\n",
      "and implementations, including workloads comprised of\n",
      "clients connected through different communication stacks.\n",
      "The framework manages underlying complexities such as\n",
      "connection handling, client life cycle, timeouts, and error\n",
      "handling in an for the researcher.\n",
      "3.4\n",
      "Virtual Client Engine\n",
      "Built into Flower is the Virtual Client Engine (VCE): a tool\n",
      "that enables the virtualization of Flower Clients to maximise\n",
      "utilization of the available hardware. Given a pool of clients,\n",
      "their respective compute and memory budgets (e.g. number\n",
      "of CPUs, VRAM requirements) and, the FL-speciﬁc hyper-\n",
      "parameters (e.g. number of clients per round), the VCE\n",
      "launches Flower Clients in a resource-aware manner. The\n",
      "VCE will schedule, instantiate and run the Flower Clients\n",
      "in a transparent way to the user and the Flower Server. This\n",
      "property greatly simpliﬁes parallelization of jobs, ensuring\n",
      "the available hardware is not underutilised and, enables port-\n",
      "ing the same FL experiment to a wide varying of setups\n",
      "without reconﬁguration: a desktop machine, a single GPU\n",
      "rack or multi-node GPU cluster. The VCE therefore be-\n",
      "comes a key module inside the Flower framework enabling\n",
      "running large scale FL workloads with minimal overhead in\n",
      "a scalable manner.\n",
      "3.5\n",
      "Edge Client Engine\n",
      "Flower is designed to be open source, extendable and,\n",
      "framework and device agnostic. Some devices suitable for\n",
      "lightweight FL workloads such as Raspberry Pi or NVIDIA\n",
      "Jetson require minimal or no special conﬁguration. These\n",
      "Python-enabled embedded devices can readily be used as\n",
      "Flower Clients. On the other hand, commodity devices such\n",
      "as smartphones require a more strict, limited and sometimes\n",
      "proprietary software stack to run ML workloads. To circum-\n",
      "Flower: A Friendly Federated Learning Framework\n",
      "Table 2. Comparison of different FL frameworks.\n",
      "TFF\n",
      "Syft\n",
      "FedScale LEAF Flower\n",
      "Single-node simulation\n",
      "√\n",
      "√\n",
      "√\n",
      "√\n",
      "√\n",
      "Multi-node execution\n",
      "*\n",
      "√\n",
      "(√)***\n",
      "√\n",
      "Scalability\n",
      "*\n",
      "**\n",
      "√\n",
      "Heterogeneous clients\n",
      "(√)***\n",
      "**\n",
      "√\n",
      "ML framework-agnostic\n",
      "****\n",
      "****\n",
      "√\n",
      "Communication-agnostic\n",
      "√\n",
      "Language-agnostic\n",
      "√\n",
      "Baselines\n",
      "√\n",
      "√\n",
      "*\n",
      "Labels: * Planned / ** Only simulated\n",
      "*** Only Python-based / **** Only PyTorch and/or TF/Keras\n",
      "vent this limitation, Flower provides a low-level integration\n",
      "by directly handling Flower Protocol messages on the client.\n",
      "3.6\n",
      "Secure Aggregation\n",
      "In FL the server does not have direct access to a client’s\n",
      "data. To further protect clients’ local data, Flower provides\n",
      "implementation of both SecAgg (Bonawitz et al., 2017) and\n",
      "SecAgg+ (Bell et al., 2020) protocols for a semi-honest\n",
      "threat model. The Flower secure aggregation implementa-\n",
      "tion satisﬁes ﬁve goals: usability, ﬂexibility, compatibility,\n",
      "reliability and efﬁciency. The execution of secure aggre-\n",
      "gation protocols is independent of any special hardware\n",
      "and ML framework, robust against client dropouts, and\n",
      "has lower theoretical overhead for both communication and\n",
      "computation than other traditional multi-party computation\n",
      "secure aggregation protocol, which will be shown in 5.5.\n",
      "3.7\n",
      "FL Framework Comparison\n",
      "We compare Flower to other FL toolkits, namely TFF\n",
      "(Google, 2020), Syft (Ryffel et al., 2018), FedScale (Lai\n",
      "et al., 2021) and LEAF (Caldas et al., 2018). Table 2 pro-\n",
      "vides an overview, with a more detailed description of those\n",
      "properties following thereafter.\n",
      "Single-node simulation enables simulation of FL systems\n",
      "on a single machine to investigate workload performance\n",
      "without the need for a multi-machine system. Supported by\n",
      "all frameworks.\n",
      "Multi-node execution requires network communication\n",
      "between server and clients on different machines. Multi-\n",
      "machine execution is currently supported by Syft and Flower.\n",
      "FedScale supports multi-machine simulation (but not real\n",
      "deployment), TFF plans multi-machine deployments.\n",
      "Scalability is important to derive experimental results that\n",
      "generalize to large cohorts. Single-machine simulation is\n",
      "limited because workloads including a large number of\n",
      "clients often exhibit vastly different properties. TFF and\n",
      "LEAF are, at the time of writing, constrained to single-\n",
      "machine simulations. FedScale can simulate clients on mul-\n",
      "tiple machines, but only scales to 100 concurrent clients.\n",
      "Syft is able to communicate over the network, but only by\n",
      "connecting to data holding clients that act as servers them-\n",
      "selves, which limits scalability. In Flower, data-holding\n",
      "clients connect to the server which allows workloads to\n",
      "scale to millions of clients, including scenarios that require\n",
      "full control over when connections are being opened and\n",
      "closed. Flower also includes a virtual client engine for\n",
      "large-scale multi-node simulations.\n",
      "Heterogeneous clients refers to the ability to run workloads\n",
      "comprised of clients running on different platforms using\n",
      "different languages, all in the same workload. FL targeting\n",
      "edge devices will clearly have to assume pools of clients\n",
      "of many different types (e.g., phone, tablet, embedded).\n",
      "Flower supports such heterogeneous client pools through\n",
      "its language-agnostic and communication-agnostic client-\n",
      "side integration points. It is the only framework in our\n",
      "comparison that does so, with TFF and Syft expecting a\n",
      "framework-provided client runtime, whereas FedScale and\n",
      "LEAF focus on Python-based simulations.\n",
      "ML framework-agnostic toolkits allow researchers and\n",
      "users to leverage their previous investments in existing ML\n",
      "frameworks by providing universal integration points. This\n",
      "is a unique property of Flower: the ML framework land-\n",
      "scape is evolving quickly (e.g., JAX (Bradbury et al., 2018),\n",
      "PyTorch Lightning (W. Falcon, 2019)) and therefore the\n",
      "user should choose which framework to use for their local\n",
      "training pipelines. TFF is tightly coupled with TensorFlow\n",
      "and experimentally supports JAX, LEAF also has a depen-\n",
      "dency on TensorFlow, and Syft provides hooks for PyTorch\n",
      "and Keras, but does not integrate with arbitrary tools.\n",
      "Language-agnostic describes the capability to implement\n",
      "clients in a variety of languages, a property especially im-\n",
      "portant for research on mobile and emerging embedded\n",
      "platforms. These platforms often do not support Python,\n",
      "but rely on speciﬁc languages (Java on Android, Swift on\n",
      "iOS) for idiomatic development, or native C++ for resource\n",
      "constrained embedded devices. Flower achieves a fully\n",
      "language-agnostic interface by offering protocol-level inte-\n",
      "gration. Other frameworks are based on Python, with some\n",
      "of them indicating a plan to support Android and iOS (but\n",
      "not embedded platforms) in the future.\n",
      "Baselines allow the comparison of existing methods with\n",
      "new FL algorithms.\n",
      "Having existing implementations\n",
      "at ones disposal can greatly accelerate research progress.\n",
      "LEAF and FedScale come with a number of benchmarks\n",
      "built-in with different datasets. TFF provides libraries for\n",
      "constructing baselines with some datasets. Flower currently\n",
      "implements a number of FL methods in the context of pop-\n",
      "ular ML benchmarks, e.g., a federated training of CIFAR-\n",
      "10 (Krizhevsky et al., 2005) image classiﬁcation, and has\n",
      "initial port of LEAF datasets such as FEMNIST and Shake-\n",
      "Flower: A Friendly Federated Learning Framework\n",
      "speare (Caldas et al., 2018).\n",
      "4\n",
      "IMPLEMENTATION\n",
      "Flower has an extensive implementation of FL averaging\n",
      "algorithms, a robust communication stack, and various ex-\n",
      "amples of deploying Flower on real and simulated clients.\n",
      "Due to space constraints, we only focus on some of the\n",
      "implementation details in this section and refer the reader\n",
      "to the Flower GitHub repository for more details.\n",
      "Communication stack. FL requires stable and efﬁcient\n",
      "communication between clients and server. The Flower\n",
      "communication protocol is currently implemented on top of\n",
      "bi-directional gRPC (Foundation) streams. gRPC deﬁnes\n",
      "the types of messages exchanged and uses compilers to then\n",
      "generate efﬁcient implementations for different languages\n",
      "such as Python, Java, or C++. A major reason for choosing\n",
      "gRPC was its efﬁcient binary serialization format, which is\n",
      "especially important on low-bandwidth mobile connections.\n",
      "Bi-directional streaming allows for the exchange of multiple\n",
      "message without the overhead incurred by re-establishing a\n",
      "connection for every request/response pair.\n",
      "Serialization. Independent of communication stack, Flower\n",
      "clients receive instructions (messages) as raw byte arrays\n",
      "(either via the network or throught other means, for exam-\n",
      "ple, inter-process communication), deserialize the instruc-\n",
      "tion, and execute the instruction (e.g., training on local\n",
      "data). The results are then serialized and communicated\n",
      "back to the server. Note that a client communicates with the\n",
      "server through language-independent messages and can thus\n",
      "be implemented in a variety of programming languages, a\n",
      "key property to enable real on-device execution. The user-\n",
      "accessible byte array abstraction makes Flower uniquely\n",
      "serialization-agnostic and enables users to experiment with\n",
      "custom serialization methods, for example, gradient com-\n",
      "pression or encryption.\n",
      "Alternative communication stacks. Even though the cur-\n",
      "rent implementation uses gRPC, there is no inherent reliance\n",
      "on it. The internal Flower server architecture uses modu-\n",
      "lar abstractions such that components that are not tied to\n",
      "gRPC are unaware of it. This enables the server to support\n",
      "user-provided RPC frameworks and orchestrate workloads\n",
      "across heterogeneous clients, with some connected through\n",
      "gRPC, and others through other RPC frameworks.\n",
      "ClientProxy. The abstraction that enables communication-\n",
      "agnostic execution is called ClientProxy. Each ClientProxy\n",
      "object registered with the ClientManager represents a single\n",
      "client that is available to the server for training or evalua-\n",
      "tion. Clients which are ofﬂine do not have an associated\n",
      "ClientProxy object. All server-side logic (client conﬁgu-\n",
      "ration, receiving results from clients) is built against the\n",
      "ClientProxy abstraction.\n",
      "One key design decision that makes Flower so ﬂexible is\n",
      "that ClientProxy is an abstract interface, not an implementa-\n",
      "tion. There are different implementations of the ClientProxy\n",
      "interface, for example, GrpcClientProxy. Each implementa-\n",
      "tion encapsulates details on how to communicate with the\n",
      "actual client, for example, to send messages to an actual\n",
      "edge device using gRPC.\n",
      "Virtual Client Engine (VCE). Resource consumption\n",
      "(CPU, GPU, RAM, VRAM, etc.) is the major bottleneck\n",
      "for large-scale experiments. Even a modestly sized model\n",
      "easily exhausts most systems if kept in memory a million\n",
      "times. The VCE enables large-scale single-machine or multi-\n",
      "machine experiments by executing workloads in a resource-\n",
      "aware fashion that either increases parallelism for better\n",
      "wall-clock time or to enable large-scale experiments on lim-\n",
      "ited hardware resources. It creates a ClientProxy for each\n",
      "client, but defers instantiation of the actual client object (in-\n",
      "cluding local model and data) until the resources to execute\n",
      "the client-side task (training, evaluation) become available.\n",
      "This avoids having to keep multiple client-side models and\n",
      "datasets in memory at any given point in time.\n",
      "VCE builds on the Ray (Moritz et al., 2018) framework to\n",
      "schedule the execution of client-side tasks. In case of lim-\n",
      "ited resources, Ray can sequence the execution of client-side\n",
      "computations, thus enabling a much larger scale of experi-\n",
      "ments on common hardware. The capability to perform FL\n",
      "at scale will unlock new research opportunities as results\n",
      "obtained in small-scale experiments often do not generalize\n",
      "well to large-cohort settings.\n",
      "5\n",
      "FRAMEWORK EVALUATION\n",
      "In this section we evaluate Flower’s capabilities in sup-\n",
      "porting both research and implementations of real-world\n",
      "FL workloads. Our evaluation focuses on three main as-\n",
      "pects:\n",
      "• Scalability: We show that Flower can (a) efﬁciently\n",
      "make use of available resources in single-machine simu-\n",
      "lations and (b) run experiments with millions of clients\n",
      "whilst sampling thousands in each training.\n",
      "• Heterogeneity: We show that Flower can be deployed\n",
      "in real, heterogeneous devices commonly found in cross-\n",
      "device scenario and how it can be used to measure sys-\n",
      "tem statistics.\n",
      "• Realism: We show through a case study how Flower can\n",
      "throw light on the performance of FL under heteroge-\n",
      "neous clients with different computational and network\n",
      "capabilities.\n",
      "• Privacy: Finally, we show how our implementation of\n",
      "Secure Aggregation matches the expected theoretical\n",
      "overhead as expected.\n",
      "Flower: A Friendly Federated Learning Framework\n",
      "5.1\n",
      "Large-Scale Experiment\n",
      "Federated Learning receives most of its power from its abil-\n",
      "ity to leverage data from millions of users. However, se-\n",
      "lecting large numbers of clients in each training round does\n",
      "not necessarily translate into faster convergence times. In\n",
      "fact, as observed in (McMahan et al., 2017), there is usually\n",
      "an empirical threshold for which if we increase the number\n",
      "of participating clients per round beyond that point, con-\n",
      "vergence will be slower. By allowing experiments to run\n",
      "at mega-scales, with thousands of active clients per round,\n",
      "Flower gives us the opportunity to empirically ﬁnd such\n",
      "threshold for any task at hand.\n",
      "To show this ability, in this series of experiments we use\n",
      "Flower to ﬁne-tune a network on data from 15M users using\n",
      "different numbers of clients per round. More speciﬁcally,\n",
      "we ﬁne-tune a Transformer network to correctly predict\n",
      "Amazon book ratings based on text reviews from users.\n",
      "Experimental Setup. We choose to use Amazon’s Book\n",
      "Reviews Dataset (Ni et al., 2019) which contains over 51M\n",
      "reviews from 15M different users. Each review from a\n",
      "given user contains a textual review of a book along with its\n",
      "given rank (1-5). We ﬁne-tune the classiﬁer of a pre-trained\n",
      "DistilBERT model (Sanh et al., 2019) to correctly predict\n",
      "ranks based on textual reviews. For each experiment we ﬁx\n",
      "the number of clients being sampled in each round (from\n",
      "10 to 1000) and aggregate models using FedAvg. We test\n",
      "the aggregated model after each round on a ﬁxed set of 1M\n",
      "clients. Convergence curves are reported in Figure 3 all our\n",
      "experiments were run using two NVIDIA V100 GPUs on a\n",
      "22-cores of an Intel Xeon Gold 6152 (2.10GHz) CPU.\n",
      "Results. Figure 3 shows the expected initial speed-up in\n",
      "convergence when selecting 10 to 500 clients per round in\n",
      "each experiment. However, if we decide to sample 1k clients\n",
      "in each round, we notice an increase in convergence time.\n",
      "Intuitively, this behaviour is caused by clients’ data having\n",
      "very different distributions; making it difﬁcult for simple\n",
      "Aggregation Strategies such as FedAvg to ﬁnd a suitable\n",
      "set of weights.\n",
      "5.2\n",
      "Single Machine Experiments\n",
      "One of our strongest claims in this paper is that Flower\n",
      "can be effectively used in Research. For this to be true,\n",
      "Flower needs to be fast at providing reliable results when\n",
      "experimenting new ideas, e.g. a new aggregation strategy.\n",
      "In this experiment, we provide a head-to-head comparison\n",
      "in term of training times between Flower and the four main\n",
      "FL frameworks, namely FedScale, TFF, FedJax and the\n",
      "original LEAF, when training with different FL setups.\n",
      "Experimental Setup. We consider all three FL setups pro-\n",
      "posed by (Caldas et al., 2018) when training a CNN model\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "Rounds\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "Accuracy\n",
      "1000 clients\n",
      "500 clients\n",
      "100 clients\n",
      "50 clients\n",
      "10 clients\n",
      "Figure 3. Flower scales to even 15M user experiments. Each curve\n",
      "shows successful convergence of the DistilBERT model under\n",
      "varying amounts of clients per round, with the exception of the\n",
      "two smallest client sizes: 50 and 10.\n",
      "to correctly classify characters from the FEMNIST dataset.\n",
      "More speciﬁcally, we consider the scenarios where the num-\n",
      "ber of clients (c) and local epochs per round change (l) vary.\n",
      "The total number of rounds and total number of clients are\n",
      "kept constant at 2000 and 179, respectively. To allow for\n",
      "a fair comparison, We run all our experiments using eight\n",
      "cores of an Intel Xeon E5-2680 CPU (2.40GHz) equipped\n",
      "with two NVIDIA RTX2080 GPUs and 20GB of RAM.\n",
      "Results. Figure 4 shows the impact of choosing different\n",
      "FL frameworks for the various tasks. On our ﬁrst task,\n",
      "when training using three clients per round (c = 3) for one\n",
      "local epoch (l = 1), FedJax ﬁnishes training ﬁrst (05:18),\n",
      "LEAF ﬁnishes second (44:39) followed by TFF (58:29) and\n",
      "Flower (59:19). In this simple case, the overhead of having\n",
      "a multi-task system, like the Virtual Client Engine (VCE),\n",
      "causes Flower to sightly under-perform in comparison to\n",
      "loop-based simulators, like LEAF.\n",
      "However, the beneﬁts of having a VCE become more evident\n",
      "if we train on more realistic scenarios. When increasing the\n",
      "number of clients per round to 35 while keeping the single\n",
      "local epoch, we notice that Flower (230:18) is still among\n",
      "the fastest frameworks. Since the number of local epochs is\n",
      "still one, most of the overhead comes from loading data and\n",
      "models into memory rather than performing real training,\n",
      "hence the similarity those LEAF and Flower.\n",
      "The VCE allows us to specify the amount of GPU memory\n",
      "we want to associate with each client, this allows for more\n",
      "efﬁcient data and model loading of different clients on the\n",
      "same GPU, making the overall training considerably faster.\n",
      "In fact, when we substantially increase the amount of work\n",
      "performed by each client to 100 local epochs, while ﬁxing\n",
      "the number of active client to 3, we see a signiﬁcant saving\n",
      "in training time. In this task Flower outperforms all other. It\n",
      "Flower: A Friendly Federated Learning Framework\n",
      "(c=3, l=1)\n",
      "(c=3, l=100)\n",
      "(c=35, l=1)\n",
      "FEMNIST Task\n",
      "10\n",
      "1\n",
      "10\n",
      "2\n",
      "10\n",
      "3\n",
      "Training Time (s)\n",
      "FedJax\n",
      "FedScale\n",
      "Flower\n",
      "LEAF\n",
      "TFF\n",
      "Figure 4. Training times (log scale in second) comparison of dif-\n",
      "ferent FEMNIST tasks between different FL frameworks.\n",
      "completes the task in just about 80 minutes, while the second\n",
      "best performing framework (FedJax) takes over twice as\n",
      "long (over 173 minutes).\n",
      "It is also important to acknowledge the two extreme training\n",
      "times we see in this experiment. FedJax seems to be very\n",
      "efﬁcient when training on few (1) local epochs; however, in\n",
      "scenarios where communication-efﬁciency is key and larger\n",
      "number of local epochs are required, FedJax performance\n",
      "slightly degrades. FedScale, on the other hands, consistently\n",
      "showed high training times across all training scenarios.\n",
      "We believe this apparent inefﬁciency to be associated with\n",
      "network overheads that are usually unnecessary in a single-\n",
      "computer simulation.\n",
      "5.3\n",
      "Flower enables FL evaluation on real devices\n",
      "Flower can assist researchers in quantifying the system costs\n",
      "associated with running FL on real devices and to identify\n",
      "bottlenecks in real-world federated training. In this section,\n",
      "we present the results of deploying Flower on six types of\n",
      "heterogeneous real-world mobile and embedded devices, in-\n",
      "cluding Java-based Android smartphones and Python-based\n",
      "Nvidia Jetson series devices and Raspberry Pi.\n",
      "Experiment Setup. We run the Flower server conﬁgured\n",
      "with the FedAvg strategy and host it on a cloud virtual\n",
      "machine. Python-based Flower clients are implemented for\n",
      "Nvidia Jetson series devices (Jetson Nano, TX2, NX, AGX)\n",
      "and Raspberry Pi, and trained using TensorFlow as the ML\n",
      "framework on each client. On the other hand, Android smart-\n",
      "phones currently do not have extensive on-device training\n",
      "support with TensorFlow or PyTorch. To counter this issue,\n",
      "we leverage TensorFlow Lite to implement Flower clients\n",
      "on Android smartphones in Java. While TFLite is primarily\n",
      "designed for on-device inference, we leverage its capabili-\n",
      "ties to do on-device model personalization to implement a\n",
      "FL client application (Lite, 2020). The source code for both\n",
      "implementations is available in the Flower repository.\n",
      "Results. Figure 5 shows the system metrics associated\n",
      "with training a DeepConvLSTM (Singh et al., 2021) model\n",
      "for a human activity recognition task on Python-enabled\n",
      "Jetson and Raspberry Pi devices. We used the RealWorld\n",
      "dataset (Sztyler & Stuckenschmidt, 2016) consisting of time-\n",
      "series data from accelerometer and gyroscope sensors on\n",
      "mobile devices, and partitioned it across 10 mobile clients.\n",
      "The ﬁrst takeaway from our experiments in that we could de-\n",
      "ploy Flower clients on these heterogeneous devices, without\n",
      "requiring any modiﬁcations in the client-side Flower code.\n",
      "The only consideration was to ensure that a compatible ML\n",
      "framework (e.g., TensorFlow) is installed on each client.\n",
      "Secondly, we show in Figure 5 how FL researchers can\n",
      "deploy and quantify the training time and energy consump-\n",
      "tion of FL on various heterogeneous devices and processors.\n",
      "Here, the FL training time is aggregated over 40 rounds,\n",
      "and includes the time taken to perform local 10 local epochs\n",
      "of SGD on the client, communicating model parameters\n",
      "between the server and the client, and updating the global\n",
      "model on the server. By comparing the relative energy\n",
      "consumption and training times across various devices, FL\n",
      "researchers can devise more informed client selection poli-\n",
      "cies that can tradeoff between FL convergence time and\n",
      "overall energy consumption. For instance, choosing Jetson\n",
      "Nano-CPU based FL clients over Raspberry Pi clients may\n",
      "increase FL convergence time by 10 minutes, however it\n",
      "reduces the overall energy consumption by almost 60%.\n",
      "Next, we illustrate how Flower can enable ﬁne-grained pro-\n",
      "ﬁling of FL on real devices. We deploy Flower on 10 An-\n",
      "droid clients to train a model with 2 convolutional layers\n",
      "and 3 fully-connected layers (Flower, 2021) on the CIFAR-\n",
      "10 dataset. TensorFlow Lite is used as the training ML\n",
      "framework on the devices. We measure the time taken for\n",
      "various FL operations, such as local SGD training, commu-\n",
      "nication between the server and client, local evaluation on\n",
      "the client, and the overhead due to the Flower framework.\n",
      "AGX-GPU\n",
      "NX-GPU\n",
      "TX2-GPU\n",
      "Nano-GPU\n",
      "AGX-CPU\n",
      "NX-CPU\n",
      "TX2-CPU\n",
      "Nano-CPU\n",
      "RPI-CPU\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "FL training time (mins)\n",
      "AGX-GPU\n",
      "NX-GPU\n",
      "TX2-GPU\n",
      "Nano-GPU\n",
      "AGX-CPU\n",
      "NX-CPU\n",
      "TX2-CPU\n",
      "Nano-CPU\n",
      "RPI-CPU\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "Total energy (J)\n",
      "FL Training Time\n",
      "Energy\n",
      "Figure 5. Flower enables quantifying the system performance of\n",
      "FL on mobile and embedded devices. Here we report the training\n",
      "times and energy consumption associated with running FL on\n",
      "CPUs and GPUs of various embedded devices.\n",
      "Flower: A Friendly Federated Learning Framework\n",
      "Google Pixel 4\n",
      "Samsung Galaxy S9\n",
      "Device Names\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "Time (in seconds)\n",
      "Flower Overhead = 0.1s\n",
      "Training\n",
      "Communication\n",
      "Evaluation\n",
      "Flower Overhead\n",
      "Figure 6. Flower enables ﬁne-grained proﬁling of FL performance\n",
      "on real devices. The framework overhead is <100ms per round.\n",
      "Table 3. Effect of computational heterogeneity on FL training\n",
      "times. Using Flower, we can compute a hardware-speciﬁc cut-\n",
      "off τ (in minutes) for each processor, and ﬁnd a balance between\n",
      "FL accuracy and training time. τ = 0 indicates no cutoff time.\n",
      "GPU\n",
      "CPU\n",
      "(τ = 0)\n",
      "CPU\n",
      "(τ = 2.23)\n",
      "CPU\n",
      "(τ = 1.99)\n",
      "Accuracy\n",
      "0.67\n",
      "0.67\n",
      "0.66\n",
      "0.63\n",
      "Training\n",
      "time (mins)\n",
      "80.32\n",
      "102\n",
      "(1.27×)\n",
      "89.15\n",
      "(1.11×)\n",
      "80.34\n",
      "(1.0×)\n",
      "The overhead includes converting model gradients to GRPC-\n",
      "compatible buffers and vice-versa, to enable communication\n",
      "between Java FL clients and a Python FL server. In Figure 6,\n",
      "we report the mean latency of various FL operations over\n",
      "40 rounds on two types of Android devices: Google Pixel 4\n",
      "and Samsung Galaxy S9. We observe that on both devices,\n",
      "local training remains the most time-consuming operation,\n",
      "and that the total system overhead of the Flower framework\n",
      "is less than 100ms per round.\n",
      "5.4\n",
      "Realism in Federated Learning\n",
      "Flower facilitates the deployment of FL on real-world de-\n",
      "vices. While this property is beneﬁcial for production-grade\n",
      "systems, can it also assist researchers in developing better\n",
      "federated optimization algorithms? In this section, we study\n",
      "two realistic scenarios of FL deployment.\n",
      "Computational Heterogeneity across Clients.\n",
      "In real-\n",
      "world, FL clients will have vastly different computational\n",
      "capabilities. While newer smartphones are now equipped\n",
      "with mobile GPUs, other phones or wearable devices may\n",
      "have a much less powerful processor. How does this com-\n",
      "putational heterogeneity impact FL?\n",
      "For this experiment, we use a Nvidia Jetson TX2 as the\n",
      "client device, which has a Pascal GPU and six CPU cores.\n",
      "We train a ResNet18 model on the CIFAR-10 dataset in a\n",
      "federated setting with 10 total Jetson TX2 clients and 40\n",
      "rounds of training. In Table 3, we observe that if Jetson TX2\n",
      "CPU clients are used for federated training (local epochs\n",
      "E=10), the FL process would take 1.27× more time to con-\n",
      "verge as compared to training on Jetson TX2 GPU clients.\n",
      "Once we obtain this quantiﬁcation of computational het-\n",
      "erogeneity using Flower, we can design better federated\n",
      "optimization algorithms. As an example, we implemented\n",
      "a modiﬁed version of FedAvg where each client device is\n",
      "assigned a cutoff time (τ) after which it must send its model\n",
      "parameters to the server, irrespective of whether it has ﬁn-\n",
      "ished its local epochs or not. This strategy has parallels with\n",
      "the FedProx algorithm (Li et al., 2018) which also accepts\n",
      "partial results from clients. However, the key advantage\n",
      "of Flower’s on-device training capabilities is that we can\n",
      "accurately measure and assign a realistic processor-speciﬁc\n",
      "cutoff time for each client. For example, we measure that\n",
      "on average it takes 1.99 minutes to complete a FL round\n",
      "on the TX2 GPU. We then set the same time as a cutoff for\n",
      "CPU clients (τ = 1.99 mins) as shown in Table 3. This\n",
      "ensures that we can obtain faster convergence even in the\n",
      "presence of CPU clients, at the expense of a 4% accuracy\n",
      "drop. With τ = 2.23, a better balance between accuracy\n",
      "and convergence time could be obtained for CPU clients.\n",
      "Heterogeneity in Network Speeds. An important consid-\n",
      "eration for any FL system is to choose a set of participating\n",
      "clients in each training round. In the real-world, clients are\n",
      "distributed across the world and vary in their download and\n",
      "upload speeds. Hence, it is critical for any FL system to\n",
      "study how client selection can impact the overall FL train-\n",
      "ing time. We now present an experiment with 40 clients\n",
      "collaborating to train a 4-layer deep CNN model for the\n",
      "FashionMNIST dataset. More details about the dataset and\n",
      "network architecture are presented in the Appendix.\n",
      "Using Flower, we instantiate 40 clients on a cloud platform\n",
      "and ﬁx the download and upload speeds for each client using\n",
      "the WONDERSHAPER library. Each client is representative\n",
      "of a country and its download and upload speed is set based\n",
      "on a recent market survey of 4G and 5G speeds in different\n",
      "countries (OpenSignal, 2020).\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "Country Index\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "FL time (in mins)\n",
      "Figure 7. Effect of network heterogeneity in clients on FL training\n",
      "time. Using this quantiﬁcation, we designed a new client sampling\n",
      "strategy called FedFS (detailed in the Appendix).\n",
      "Flower: A Friendly Federated Learning Framework\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "·105\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "Model vector size\n",
      "CPU running time (s)\n",
      "0% dropout\n",
      "5% dropout\n",
      "Figure 8. Performance of Secure Aggregation. Running time of\n",
      "server with increasing vector size\n",
      "The x-axis of Figure 7 shows countries arranged in descend-\n",
      "ing order of their network speeds: country indices 1-20\n",
      "represent the top 20 countries based on their network speeds\n",
      "(mean download speed = 40.1Mbps), and indices 21-40 are\n",
      "the bottom 20 countries (mean download speed = 6.76Mbps).\n",
      "We observe that if all clients have the network speeds corre-\n",
      "sponding to Country 1 (Canada), the FL training ﬁnishes in\n",
      "8.9 mins. As we include slower clients in FL, the training\n",
      "time gradually increases, with a major jump around index =\n",
      "17. On the other extreme, for client speeds corresponding to\n",
      "Country 40 (Iraq), the FL training takes 108 minutes.\n",
      "There are two key takeaways from this experiment: a) Using\n",
      "Flower, we can proﬁle the training time of any FL algorithm\n",
      "under scenarios of network heterogeneity, b) we can lever-\n",
      "age these insights to design sophisticated client sampling\n",
      "techniques. For example, during subsequent rounds of feder-\n",
      "ated learning, we could monitor the number of samples each\n",
      "client was able to process during a given time window and\n",
      "increase the selection probability of slow clients to balance\n",
      "the contributions of fast and slow clients to the global model.\n",
      "The FedFS strategy detailed in the appendix works on this\n",
      "general idea, and reduces the convergence time of FL by up\n",
      "to 30% over the FedAvg random sampling approach.\n",
      "5.5\n",
      "Secure Aggregation Overheads\n",
      "Privacy is one of the cornerstones in FL, which inevitably\n",
      "generates computational overhead during training.\n",
      "In\n",
      "hardware-constrained systems, such as cross-device FL, it\n",
      "is desirable not only to be able to measure such overheads,\n",
      "but also to make sure that security protocols are well imple-\n",
      "mented and follow the expected protocol described in the\n",
      "original papers. Flower’s implementation of Secure Aggre-\n",
      "gation, named Salvia, is based on the SecAgg (Bonawitz\n",
      "et al., 2017) and SecAgg+ (Bell et al., 2020) protocols as\n",
      "described in Section 3.6. To verify that Salvia’s behavior\n",
      "matches the expected theoretical complexity, we evaluate\n",
      "its impact on server-side computation and communication\n",
      "overhead with the model vector size and clients dropouts.\n",
      "Experiment Setup. The FL simulations run on a Linux\n",
      "system with an Intel Xeon E-2136 CPU (3.30GHz), with\n",
      "256 GB of RAM. In our simulations, all entries of our local\n",
      "vectors are of size 24 bits. We ignore communication la-\n",
      "tency. Moreover, all dropouts simulated happen after stage\n",
      "2, i.e. Share Keys Stage. This is because this imposes the\n",
      "most signiﬁcant overhead as the server not only needs to\n",
      "regenerate dropped-out clients’ secrets, but also compute\n",
      "their pairwise masks generated between their neighbours.\n",
      "For our simulations, the n and t parameters of the t-out-\n",
      "of-n secret-sharing scheme are set to 51 and 26, respec-\n",
      "tively. These parameters are chosen to reference SecAgg+’s\n",
      "proven correctness and security guarantees, where we can\n",
      "tolerate up to 5% dropouts and 5% corrupted clients with\n",
      "correctness holding with probability 1 −2−20 and security\n",
      "holding with probability 1 −2−40.\n",
      "Results. Fixing the number of sampled clients to 100, we\n",
      "plotted CPU running times through aggregating a vector\n",
      "of size 100k entries to aggregating one of size 500k en-\n",
      "tries in Figure 8. We also measured how the performance\n",
      "would change after client dropouts by repeating the same\n",
      "experiments with a 5% client dropout.\n",
      "Both the running times and total data transfer of the server\n",
      "increase linearly with the model vector size as the operations\n",
      "involving model vectors are linear to the vectors’ sizes,\n",
      "e.g. generating masks, sending vectors. We also note the\n",
      "server’s running time increases when there are 5% clients\n",
      "dropping out, as the server has to perform extra computation\n",
      "to calculate all k pairwise masks for each client dropped.\n",
      "Lastly, we observe that the total data transferred of the server\n",
      "remains unchanged with client dropouts as each client only\n",
      "communicates with the server plus exactly k neighbors,\n",
      "regardless of the total number of clients and dropouts. We\n",
      "conclude that all our experimental data matches the expected\n",
      "complexities of SecAgg and SecAgg+.\n",
      "6\n",
      "CONCLUSION\n",
      "We have presented Flower – a novel framework that is specif-\n",
      "ically designed to advance FL research by enabling hetero-\n",
      "geneous FL workloads at scale. Although Flower is broadly\n",
      "useful across a range of FL settings, we believe that it will be\n",
      "a true game-changer for reducing the disparity between FL\n",
      "research and real-world FL systems. Through the provided\n",
      "abstractions and components, researchers can federated ex-\n",
      "isting ML workloads (regardless of the ML framework used)\n",
      "and transition these workloads from large-scale simulation\n",
      "to execution on heterogeneous edge devices. We further\n",
      "evaluate the capabilities of Flower in experiments that target\n",
      "both scale and systems heterogeneity by scaling FL up to\n",
      "15M clients, providing head-to-head comparison between\n",
      "different FL frameworks for single-computer experiments,\n",
      "measuring FL energy consumption on a cluster of Nvidia\n",
      "Flower: A Friendly Federated Learning Framework\n",
      "Jetson TX2 devices, optimizing convergence time under\n",
      "limited bandwidth, and illustrating a deployment of Flower\n",
      "on a range of Android mobile devices in the AWS Device\n",
      "Farm. Flower is open-sourced under Apache 2.0 License\n",
      "and we look forward to more community contributions to it.\n",
      "REFERENCES\n",
      "Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A.,\n",
      "Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard,\n",
      "M., Kudlur, M., Levenberg, J., Monga, R., Moore, S.,\n",
      "Murray, D. G., Steiner, B., Tucker, P., Vasudevan, V.,\n",
      "Warden, P., Wicke, M., Yu, Y., and Zheng, X. Tensorﬂow:\n",
      "A system for large-scale machine learning.\n",
      "In 12th\n",
      "USENIX Symposium on Operating Systems Design and\n",
      "Implementation (OSDI 16), pp. 265–283, 2016a. URL\n",
      "https://www.usenix.org/system/files/\n",
      "conference/osdi16/osdi16-abadi.pdf.\n",
      "Abadi, M., Chu, A., Goodfellow, I., McMahan, B., Mironov,\n",
      "I., Talwar, K., and Zhang, L. Deep learning with dif-\n",
      "ferential privacy.\n",
      "In 23rd ACM Conference on Com-\n",
      "puter and Communications Security (ACM CCS), pp.\n",
      "308–318, 2016b. URL https://arxiv.org/abs/\n",
      "1607.00133.\n",
      "Bell, J. H., Bonawitz, K. A., Gasc´on, A., Lepoint, T., and\n",
      "Raykova, M. Secure single-server aggregation with (poly)\n",
      "logarithmic overhead. In Proceedings of the 2020 ACM\n",
      "SIGSAC Conference on Computer and Communications\n",
      "Security, pp. 1253–1269, 2020.\n",
      "Bhagoji, A. N., Chakraborty, S., Mittal, P., and Calo, S. B.\n",
      "Analyzing federated learning through an adversarial lens.\n",
      "CoRR, abs/1811.12470, 2018. URL http://arxiv.\n",
      "org/abs/1811.12470.\n",
      "Bonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A.,\n",
      "McMahan, H. B., Patel, S., Ramage, D., Segal, A.,\n",
      "and Seth, K. Practical secure aggregation for privacy-\n",
      "preserving machine learning. In Proceedings of the 2017\n",
      "ACM SIGSAC Conference on Computer and Communi-\n",
      "cations Security, CCS ’17, pp. 1175–1191, New York,\n",
      "NY, USA, 2017. ACM. ISBN 978-1-4503-4946-8. doi:\n",
      "10.1145/3133956.3133982. URL http://doi.acm.\n",
      "org/10.1145/3133956.3133982.\n",
      "Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Inger-\n",
      "man, A., Ivanov, V., Kiddon, C. M., Koneˇcn´y, J., Maz-\n",
      "zocchi, S., McMahan, B., Overveldt, T. V., Petrou, D.,\n",
      "Ramage, D., and Roselander, J. Towards federated learn-\n",
      "ing at scale: System design. In SysML 2019, 2019. URL\n",
      "https://arxiv.org/abs/1902.01046. To ap-\n",
      "pear.\n",
      "Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary,\n",
      "C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J.,\n",
      "Wanderman-Milne, S., and Zhang, Q. JAX: composable\n",
      "transformations of Python+NumPy programs, 2018. URL\n",
      "http://github.com/google/jax.\n",
      "Caldas, S., Duddu, S. M. K., Wu, P., Li, T., Koneˇcn`y, J.,\n",
      "McMahan, H. B., Smith, V., and Talwalkar, A. Leaf:\n",
      "A benchmark for federated settings.\n",
      "arXiv preprint\n",
      "arXiv:1812.01097, 2018.\n",
      "Chahal, K. S., Grover, M. S., and Dey, K. A hitchhiker’s\n",
      "guide on distributed training of deep neural networks.\n",
      "CoRR, abs/1810.11787, 2018. URL http://arxiv.\n",
      "org/abs/1810.11787.\n",
      "Chowdhery, A., Warden, P., Shlens, J., Howard, A.,\n",
      "and Rhodes, R.\n",
      "Visual wake words dataset.\n",
      "CoRR,\n",
      "abs/1906.05721, 2019. URL http://arxiv.org/\n",
      "abs/1906.05721.\n",
      "Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M.,\n",
      "Le, Q. V., Mao, M. Z., Ranzato, M., Senior, A., Tucker, P.,\n",
      "Yang, K., and Ng, A. Y. Large scale distributed deep net-\n",
      "works. In Proceedings of the 25th International Confer-\n",
      "ence on Neural Information Processing Systems - Volume\n",
      "1, NIPS’12, pp. 1223–1231, USA, 2012. Curran Asso-\n",
      "ciates Inc. URL http://dl.acm.org/citation.\n",
      "cfm?id=2999134.2999271.\n",
      "Dryden, N., Jacobs, S. A., Moon, T., and Van Essen, B.\n",
      "Communication quantization for data-parallel training of\n",
      "deep neural networks. In Proceedings of the Workshop\n",
      "on Machine Learning in High Performance Computing\n",
      "Environments, MLHPC ’16, pp. 1–8, Piscataway, NJ,\n",
      "USA, 2016. IEEE Press. ISBN 978-1-5090-3882-4. doi:\n",
      "10.1109/MLHPC.2016.4. URL https://doi.org/\n",
      "10.1109/MLHPC.2016.4.\n",
      "Flower.\n",
      "Model\n",
      "architecture\n",
      "for\n",
      "android\n",
      "devices.\n",
      "https://github.com/adap/flower/\n",
      "blob/main/examples/android/tflite_\n",
      "convertor/convert_to_tflite.py, 2021.\n",
      "Foundation, C. N. C. grpc: A high performance, open-\n",
      "source universal rpc framework. URL https://grpc.\n",
      "io. Accessed: 2020-03-25.\n",
      "Fromm, J., Patel, S., and Philipose, M. Heterogeneous\n",
      "bitwidth binarization in convolutional neural networks.\n",
      "In Proceedings of the 32nd International Conference on\n",
      "Neural Information Processing Systems, NIPS’18, pp.\n",
      "4010–4019, Red Hook, NY, USA, 2018. Curran Asso-\n",
      "ciates Inc.\n",
      "Georgiev, P., Lane, N. D., Mascolo, C., and Chu, D. Accel-\n",
      "erating mobile audio sensing algorithms through on-chip\n",
      "GPU ofﬂoading. In Choudhury, T., Ko, S. Y., Camp-\n",
      "bell, A., and Ganesan, D. (eds.), Proceedings of the\n",
      "Flower: A Friendly Federated Learning Framework\n",
      "15th Annual International Conference on Mobile Sys-\n",
      "tems, Applications, and Services, MobiSys’17, Niagara\n",
      "Falls, NY, USA, June 19-23, 2017, pp. 306–318. ACM,\n",
      "2017. doi: 10.1145/3081333.3081358. URL https:\n",
      "//doi.org/10.1145/3081333.3081358.\n",
      "Google. Tensorﬂow federated: Machine learning on decen-\n",
      "tralized data.\n",
      "https://www.tensorflow.org/\n",
      "federated, 2020. accessed 25-Mar-20.\n",
      "Hard, A., Rao, K., Mathews, R., Ramaswamy, S., Beaufays,\n",
      "F., Augenstein, S., Eichner, H., Kiddon, C., and Ramage,\n",
      "D. Federated learning for mobile keyboard prediction,\n",
      "2019.\n",
      "Jia, X., Song, S., He, W., Wang, Y., Rong, H., Zhou, F.,\n",
      "Xie, L., Guo, Z., Yang, Y., Yu, L., Chen, T., Hu, G., Shi,\n",
      "S., and Chu, X. Highly scalable deep learning training\n",
      "system with mixed-precision: Training imagenet in four\n",
      "minutes. CoRR, abs/1807.11205, 2018. URL http:\n",
      "//arxiv.org/abs/1807.11205.\n",
      "Krizhevsky, A., Nair, V., and Hinton, G.\n",
      "Cifar-\n",
      "10 (canadian institute for advanced research).\n",
      "On-\n",
      "line, 2005. URL http://www.cs.toronto.edu/\n",
      "˜kriz/cifar.html.\n",
      "Lai, F., Dai, Y., Zhu, X., and Chowdhury, M. Fedscale:\n",
      "Benchmarking model and system performance of feder-\n",
      "ated learning. arXiv preprint arXiv:2105.11367, 2021.\n",
      "Lee, T., Lin, Z., Pushp, S., Li, C., Liu, Y., Lee, Y.,\n",
      "Xu, F., Xu, C., Zhang, L., and Song, J. Occlumency:\n",
      "Privacy-preserving remote deep-learning inference us-\n",
      "ing sgx.\n",
      "In The 25th Annual International Confer-\n",
      "ence on Mobile Computing and Networking, MobiCom\n",
      "’19, New York, NY, USA, 2019. Association for Com-\n",
      "puting Machinery.\n",
      "ISBN 9781450361699.\n",
      "doi: 10.\n",
      "1145/3300061.3345447. URL https://doi.org/\n",
      "10.1145/3300061.3345447.\n",
      "Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A.,\n",
      "and Smith, V. Federated optimization in heterogeneous\n",
      "networks. arXiv preprint arXiv:1812.06127, 2018.\n",
      "Li, T., Sanjabi, M., and Smith, V. Fair resource allocation\n",
      "in federated learning. arXiv preprint arXiv:1905.10497,\n",
      "2019.\n",
      "Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A.,\n",
      "and Smith, V. Federated optimization in heterogeneous\n",
      "networks, 2020.\n",
      "LiKamWa, R., Hou, Y., Gao, J., Polansky, M., and Zhong,\n",
      "L. Redeye: Analog convnet image sensor architecture\n",
      "for continuous mobile vision.\n",
      "In Proceedings of the\n",
      "43rd International Symposium on Computer Architec-\n",
      "ture, ISCA ’16, pp. 255–266. IEEE Press, 2016. ISBN\n",
      "9781467389471.\n",
      "doi: 10.1109/ISCA.2016.31.\n",
      "URL\n",
      "https://doi.org/10.1109/ISCA.2016.31.\n",
      "Lite,\n",
      "T.\n",
      "On-device\n",
      "model\n",
      "personalization.\n",
      "https://blog.tensorflow.org/2019/12/\n",
      "example-on-device-model-personalization.\n",
      "html, 2020.\n",
      "Malekzadeh, M., Athanasakis, D., Haddadi, H., and Livshits,\n",
      "B. Privacy-preserving bandits, 2019.\n",
      "McMahan, B., Moore, E., Ramage, D., Hampson, S.,\n",
      "and y Arcas, B. A.\n",
      "Communication-efﬁcient learn-\n",
      "ing of deep networks from decentralized data.\n",
      "In\n",
      "Singh, A. and Zhu, X. J. (eds.), Proceedings of the\n",
      "20th International Conference on Artiﬁcial Intelligence\n",
      "and Statistics, AISTATS 2017, 20-22 April 2017, Fort\n",
      "Lauderdale, FL, USA, volume 54 of Proceedings of\n",
      "Machine Learning Research, pp. 1273–1282. PMLR,\n",
      "2017. URL http://proceedings.mlr.press/\n",
      "v54/mcmahan17a.html.\n",
      "Moritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R.,\n",
      "Liang, E., Elibol, M., Yang, Z., Paul, W., Jordan, M. I.,\n",
      "and Stoica, I. Ray: A distributed framework for emerging\n",
      "ai applications, 2018.\n",
      "Ni, J., Li, J., and McAuley, J. Justifying recommendations\n",
      "using distantly-labeled reviews and ﬁne-grained aspects.\n",
      "In Proceedings of the 2019 Conference on Empirical\n",
      "Methods in Natural Language Processing and the 9th\n",
      "International Joint Conference on Natural Language Pro-\n",
      "cessing (EMNLP-IJCNLP), pp. 188–197, 2019.\n",
      "OpenSignal.\n",
      "The state of mobile network experi-\n",
      "ence 2020:\n",
      "One year into the 5g era.\n",
      "https:\n",
      "//www.opensignal.com/reports/2020/05/\n",
      "global-state-of-the-mobile-network,\n",
      "2020. accessed 10-Oct-20.\n",
      "Reddi, S., Charles, Z., Zaheer, M., Garrett, Z., Rush, K.,\n",
      "Koneˇcn´y, J., Kumar, S., and McMahan, H. B. Adaptive\n",
      "federated optimization, 2021.\n",
      "Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,\n",
      "Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\n",
      "M., et al. Imagenet large scale visual recognition chal-\n",
      "lenge. International journal of computer vision, 115(3):\n",
      "211–252, 2015.\n",
      "Ryffel, T., Trask, A., Dahl, M., Wagner, B., Mancuso,\n",
      "J., Rueckert, D., and Passerat-Palmbach, J. A generic\n",
      "framework for privacy preserving deep learning. CoRR,\n",
      "abs/1811.04017, 2018. URL http://arxiv.org/\n",
      "abs/1811.04017.\n",
      "Sanh, V., Debut, L., Chaumond, J., and Wolf, T. Distilbert,\n",
      "a distilled version of bert: smaller, faster, cheaper and\n",
      "lighter. arXiv preprint arXiv:1910.01108, 2019.\n",
      "Flower: A Friendly Federated Learning Framework\n",
      "Sergeev, A. and Balso, M. D.\n",
      "Horovod:\n",
      "fast and\n",
      "easy distributed deep learning in tensorﬂow.\n",
      "CoRR,\n",
      "abs/1802.05799, 2018. URL http://arxiv.org/\n",
      "abs/1802.05799.\n",
      "Singh, S. P., Sharma, M. K., Lay-Ekuakille, A., Gang-\n",
      "war, D., and Gupta, S.\n",
      "Deep convlstm with self-\n",
      "attention for human activity decoding using wearable\n",
      "sensors. IEEE Sensors Journal, 21(6):8575–8582, Mar\n",
      "2021.\n",
      "ISSN 2379-9153.\n",
      "doi:\n",
      "10.1109/jsen.2020.\n",
      "3045135. URL http://dx.doi.org/10.1109/\n",
      "JSEN.2020.3045135.\n",
      "Smith, V., Chiang, C.-K., Sanjabi, M., and Talwalkar, A. S.\n",
      "Federated multi-task learning. In Advances in Neural\n",
      "Information Processing Systems, pp. 4424–4434, 2017.\n",
      "Sztyler, T. and Stuckenschmidt, H. On-body localization\n",
      "of wearable devices: An investigation of position-aware\n",
      "activity recognition. In 2016 IEEE International Con-\n",
      "ference on Pervasive Computing and Communications\n",
      "(PerCom), pp. 1–9. IEEE Computer Society, 2016. doi:\n",
      "10.1109/PERCOM.2016.7456521.\n",
      "W. Falcon, e. a.\n",
      "Pytorch lightning, 2019.\n",
      "URL\n",
      "https://github.com/williamFalcon/\n",
      "pytorch-lightning.\n",
      "Xiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a\n",
      "novel image dataset for benchmarking machine learning\n",
      "algorithms. arXiv preprint arXiv:1708.07747, 2017.\n",
      "Xie, C., Koyejo, S., and Gupta, I.\n",
      "Zeno: Distributed\n",
      "stochastic gradient descent with suspicion-based fault-\n",
      "tolerance. In Chaudhuri, K. and Salakhutdinov, R. (eds.),\n",
      "Proceedings of the 36th International Conference on Ma-\n",
      "chine Learning, volume 97 of Proceedings of Machine\n",
      "Learning Research, pp. 6893–6901, Long Beach, Cali-\n",
      "fornia, USA, 09–15 Jun 2019. PMLR. URL http://\n",
      "proceedings.mlr.press/v97/xie19b.html.\n",
      "Yao, Y., Li, H., Zheng, H., and Zhao, B. Y. Latent back-\n",
      "door attacks on deep neural networks. In Proceedings\n",
      "of the 2019 ACM SIGSAC Conference on Computer\n",
      "and Communications Security, CCS ’19, pp. 2041–2055,\n",
      "New York, NY, USA, 2019. Association for Comput-\n",
      "ing Machinery. ISBN 9781450367479. doi: 10.1145/\n",
      "3319535.3354209.\n",
      "URL https://doi.org/10.\n",
      "1145/3319535.3354209.\n",
      "A\n",
      "APPENDIX\n",
      "A.1\n",
      "Survey on papers\n",
      "From a systems perspective, a major bottleneck to FL re-\n",
      "search is the paucity of frameworks that support scalable\n",
      "<10\n",
      "100\n",
      "1k\n",
      "10k\n",
      "100k\n",
      "1M\n",
      ">1M\n",
      "total # clients in the pool\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "# papers\n",
      "35\n",
      "85\n",
      "16\n",
      "10\n",
      "2\n",
      "2\n",
      "0\n",
      "Figure 9. Histograms of the number of total FL clients used in FL\n",
      "research papers in the last two years. A vast majority of papers\n",
      "only use up to 100 clients.\n",
      "execution of FL methods on mobile and edge devices. Fig.\n",
      "9 shows the histograms of total number of clients in the FL\n",
      "pools in research papers. The research papers is gathered\n",
      "from Google Scholar that is related to federated learning\n",
      "from last 2 years which consists of total 150 papers in the\n",
      "survey. We excluded papers that are using the framework\n",
      "not available to reproduced the results. As we can see from\n",
      "the histogram, the majority of experiments only use up to\n",
      "100 total clients, which usually on datasets such as CIFAR10\n",
      "and ImageNet. There are only 3 papers using the dataset\n",
      "with a total clients pool up to 1 millions, and they are using\n",
      "the Reddit and Sentiment140 dataset from leaf (Caldas et al.,\n",
      "2018).\n",
      "A.2\n",
      "FedFS Algorithm\n",
      "We introduce Federating: Fast and Slow (FedFS) to over-\n",
      "comes the challenges arising from heterogeneous devices\n",
      "and non-IID data. FedFS acknowledges the difference in\n",
      "compute capabilities inherent in networks of mobile devices\n",
      "by combining partial work, importance sampling, and dy-\n",
      "namic timeouts to enable clients to contribute equally to the\n",
      "global model.\n",
      "Partial work. Given a (local) data set of size mk on client\n",
      "k, a batch size of B, and the number of local training\n",
      "epochs E, FedAvg performs E mk\n",
      "B (local) gradient updates\n",
      "θk ←θk −η▽ℓ(b; θk) before returning θk to the server.\n",
      "The asynchronous setting treats the success of local update\n",
      "computation as binary. If a client succeeds in computing\n",
      "E mk\n",
      "B mini-batch updates before reaching a timeout ∆, their\n",
      "weight update is considered by the server, otherwise it is dis-\n",
      "carded. The server then averages all successful θk∈{0,..,K}\n",
      "updates, weighted by mk, the number of training examples\n",
      "on client k.\n",
      "Flower: A Friendly Federated Learning Framework\n",
      "This is wasteful because a clients’ computation might be\n",
      "discarded upon reaching ∆even if it was close to computing\n",
      "the full E mk\n",
      "B gradient updates. We therefore apply the\n",
      "concept of partial work (Li et al., 2018) in which a client\n",
      "submits their locally updated θk upon reaching ∆along with\n",
      "ck, the number of examples actually involved in computing\n",
      "θk, even if ck < E mk\n",
      "B B. The server averages by ck, not\n",
      "mk, because ck can vary over different rounds and devices\n",
      "depending on a number of factors (device speed, concurrent\n",
      "processes, ∆, mk, etc.).\n",
      "Intuitively, this leads to more graceful performance degra-\n",
      "dation with smaller values for ∆. Even if ∆is set to an\n",
      "adversarial value just below the completion time of the\n",
      "fastest client, which would cause FedAvg to not consider\n",
      "any update and hence prevent convergence, FedFS would\n",
      "still progress by combining K partial updates. More im-\n",
      "portantly it allows devices which regularly discard their\n",
      "updates because of lacking compute capabilities to have\n",
      "their updates represented in the global model, which would\n",
      "otherwise overﬁt the data distribution on the subset of faster\n",
      "devices in the population.\n",
      "Importance sampling. Partial work enables FedFS to lever-\n",
      "age the observed values for cr\n",
      "k (with r ∈{1, ..., t}, the\n",
      "amount of work done by client k during all previous rounds\n",
      "up to the current round t) and Ermk (with r ∈{1, ..., t},\n",
      "the amount of work client k was maximally allowed to do\n",
      "during those rounds) for client selection during round t + 1.\n",
      "c and m can be measured in different ways depending on the\n",
      "use case. In vision, ct\n",
      "k could capture the number of image\n",
      "examples processed, whereas in speech ct\n",
      "k could measure\n",
      "the accumulated duration of all audio samples used for train-\n",
      "ing on client k during round t. ct\n",
      "k < Etmk suggests that\n",
      "client k was not able to compute Et mk\n",
      "B gradient updates\n",
      "within ∆t, so its weight update θt\n",
      "k has less of an impact on\n",
      "the global model θ compared to an update from client j with\n",
      "ct\n",
      "j = Etmj. FedFS uses importance sampling for client\n",
      "selection to mitigate the effects introduced by this differ-\n",
      "ence in client capabilities. We deﬁne the work contribution\n",
      "wk of client k as the ratio between the actual work done\n",
      "during previous rounds ck = Pt\n",
      "r=1 cr\n",
      "k and the maximum\n",
      "work possible ˆck = Pt\n",
      "r=1 Ermk. Clients which have never\n",
      "been selected before (and hence have no contribution his-\n",
      "tory) have wk = 0. We then sample clients on the selection\n",
      "probability 1−wk +ϵ (normalized over all k ∈{1, ..., K}),\n",
      "with ϵ being the minimum client selection probability. ϵ\n",
      "is an important hyper-parameter that prevents clients with\n",
      "ct\n",
      "k = Etmk to be excluded from future rounds. Basing the\n",
      "client selection probability on a clients’ previous contribu-\n",
      "tions (wk) allows clients which had low contributions in\n",
      "previous rounds to be selected more frequently, and hence\n",
      "contribute additional updates to the global model. Syn-\n",
      "chronous FedAvg is a special case of FedFS: if all clients are\n",
      "able to compute ct\n",
      "k = Etmk every round, then there will be\n",
      "Algorithm 1: FedFS\n",
      "begin Server T, C, K, ϵ, rf, rs, ∆max, E, B,\n",
      "initialise θ0\n",
      "for round t ←0, ..., T −1 do\n",
      "j ←max(⌊C · K⌋, 1)\n",
      "St ←(sample j distinct indices from {1, ..., K}\n",
      "with 1 −wk + ϵ)\n",
      "if fast round (rf, rs) then\n",
      "∆t = ∆f\n",
      "else\n",
      "∆t = ∆s\n",
      "end\n",
      "for k ∈St do in parallel\n",
      "θk\n",
      "t+1, ck, mk ←ClientTraining(k, ∆t, θt,\n",
      "E, B, ∆t)\n",
      "end\n",
      "cr ←P\n",
      "k∈St ck\n",
      "θt+1 ←P\n",
      "k∈St\n",
      "ck\n",
      "cr θk\n",
      "t+1\n",
      "end\n",
      "end\n",
      "1 2 3 4 5\n",
      "10\n",
      "15\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "Training time (Days)\n",
      "Accuracy (%)\n",
      "Top-1 Accuracy\n",
      "Top-5 Accuracy\n",
      "Figure 10. Training time reported in days and accuracies (Top-1\n",
      "and Top-5) for an ImageNet federated training with Flower.\n",
      "no difference in wk and FedFS samples amongst all clients\n",
      "with a uniform client selection probability of 1\n",
      "k.\n",
      "Alternating timeout. Gradual failure for clients which are\n",
      "not able to compute Et mk\n",
      "B gradient updates within ∆t and\n",
      "client selection based on previous contributions allow FedFS\n",
      "to use more aggressive values for ∆. One strategy is to use\n",
      "an alternating schedule for ∆in which we perform rf “fast”\n",
      "rounds with small ∆f) and rs “slow” rounds with larger\n",
      "∆s. This allows FedFS to be conﬁgured for either improved\n",
      "convergence in terms of wall-clock time or better overall\n",
      "performance (e.g., in terms for classiﬁcation accuracy).\n",
      "FedFS algorithm. The full FedFS algorithm is given in\n",
      "Algorithm 1.\n",
      "A.3\n",
      "Scaling FedAvg to ImageNet-scale datasets\n",
      "We now demonstrate that Flower can not only scale to a\n",
      "large number of clients, but it can also support training of\n",
      "FL models on web-scale workloads such as ImageNet. To\n",
      "the best of our knowledge, this is the ﬁrst-ever attempt at\n",
      "training ImageNet in a FL setting.\n",
      "Experiment Setup. We use the ILSVRC-2012 ImageNet\n",
      "Flower: A Friendly Federated Learning Framework\n",
      "partitioning (Russakovsky et al., 2015) that contains 1.2M\n",
      "pictures for training and a subset composed of 50K images\n",
      "for testing. We train a ResNet-18 model on this dataset\n",
      "in a federated setting with 50 clients equipped with four\n",
      "physical CPU cores. To this end, we partition the ImageNet\n",
      "training set into 50 IID partitions and distribute them on\n",
      "each client. During training, we also consider a simple\n",
      "image augmentation scheme based on random horizontal\n",
      "ﬂipping and cropping.\n",
      "Results. Figure 10 shows the results on the test set of\n",
      "ImageNet obtained by training a ResNet-18 model. It is\n",
      "worth to mention that based on 50 clients and 3 local epochs,\n",
      "the training lasted for about 15 days demonstrating Flower’s\n",
      "potential to run long-term and realistic experiments.\n",
      "We measured top-1 and top-5 accuracies of 59.1% and\n",
      "80.4% respectively obtained with FL compared to 63% and\n",
      "84% for centralised training. First, it is clear from Figure\n",
      "10 that FL accuracies could have increased a bit further at\n",
      "the cost of a longer training time, certainly reducing the gap\n",
      "with centralised training. Then, the ResNet-18 architecture\n",
      "relies heavily on batch-normalisation, and it is unclear how\n",
      "the internal statistics of this technique behave in the context\n",
      "of FL, potentially harming the ﬁnal results. As expected,\n",
      "the scalability of Flower helps with raising and investing\n",
      "new issues related to federated learning.\n",
      "For such long-term experiments, one major risk is that client\n",
      "devices may go ofﬂine during training, thereby nullifying\n",
      "the training progress. Flower’s built-in support for keeping\n",
      "the model states on the server and resuming the federated\n",
      "training from the last saved state in the case of failures came\n",
      "handy for this experiment.\n",
      "A.4\n",
      "Datasets and Network Architectures\n",
      "We use the following datasets and network architectures for\n",
      "our experiments.\n",
      "CIFAR-10 consists of 60,000 images from 10 different ob-\n",
      "ject classes. The images are 32 x 32 pixels in size and in\n",
      "RGB format. We use the training and test splits provided by\n",
      "the dataset authors — 50,000 images are used as training\n",
      "data and remaining 10,000 images are reserved for testing.\n",
      "Fashion-MNIST consists of images of fashion items\n",
      "(60,000 training, 10,000 test) with 10 classes such as\n",
      "trousers or pullovers. The images are 28 x 28 pixels in\n",
      "size and in grayscale format. We use a 2-layer CNN fol-\n",
      "lowed by 2 fully-connected layers for training a model on\n",
      "this dataset.\n",
      "ImageNet.\n",
      "We use the ILSVRC-2012 ImageNet (Rus-\n",
      "sakovsky et al., 2015) containing 1.2M images for training\n",
      "and 50K images for testing. A ResNet-18 model is used for\n",
      "federated training this dataset.\n",
      "' metadata={'Published': '2022-03-05', 'Title': 'Flower: A Friendly Federated Learning Research Framework', 'Authors': 'Daniel J. Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Javier Fernandez-Marques, Yan Gao, Lorenzo Sani, Kwing Hei Li, Titouan Parcollet, Pedro Porto Buarque de Gusmão, Nicholas D. Lane', 'Summary': 'Federated Learning (FL) has emerged as a promising technique for edge devices\\nto collaboratively learn a shared prediction model, while keeping their\\ntraining data on the device, thereby decoupling the ability to do machine\\nlearning from the need to store the data in the cloud. However, FL is difficult\\nto implement realistically, both in terms of scale and systems heterogeneity.\\nAlthough there are a number of research frameworks available to simulate FL\\nalgorithms, they do not support the study of scalable FL workloads on\\nheterogeneous edge devices.\\n  In this paper, we present Flower -- a comprehensive FL framework that\\ndistinguishes itself from existing platforms by offering new facilities to\\nexecute large-scale FL experiments and consider richly heterogeneous FL device\\nscenarios. Our experiments show Flower can perform FL experiments up to 15M in\\nclient size using only a pair of high-end GPUs. Researchers can then seamlessly\\nmigrate experiments to real devices to examine other parts of the design space.\\nWe believe Flower provides the community with a critical new tool for FL study\\nand development.'}\n"
     ]
    }
   ],
   "source": [
    "print(arx_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da8b7b2",
   "metadata": {},
   "source": [
    "WikipediaLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a01d2d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "wiki_loader = WikipediaLoader(query=\"Taylor Swift\", load_max_docs=10)\n",
    "wiki_docs = wiki_loader.load()\n",
    "print(len(wiki_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e61f6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Folklore is the eighth studio album by the American singer-songwriter Taylor Swift. It was surprise-released on July 24, 2020, by Republic Records. Conceived during quarantine in early 2020, amidst the COVID-19 pandemic, the album explores themes of escapism, nostalgia, and romanticism. Swift recorded her vocals in her Los Angeles home studio and worked virtually with the producers Aaron Dessner and Jack Antonoff, who operated from their studios in the Hudson Valley and New York City.  \n",
      "Using a set of characters and story arcs to depict fictional narratives, the album departs from the autobiographical songwriting that had characterized Swift's past albums. Experimenting with new musical styles, Folklore consists of mellow ballads driven by piano, strings, and muted percussion; music critics classify the genre as a blend of folk, pop, alternative, electronic, and rock subgenres. The album's title was inspired by the lasting legacy of folktales, and its visual aesthetic adopts a cottagecore style.\n",
      "Folklore was accompanied by the concert documentary Folklore: The Long Pond Studio Sessions, featuring Swift's commentary and performances. The album topped the charts in Australasia and various European countries and was certified platinum or higher in Australia, Austria, Denmark, Italy, New Zealand, Norway, Poland, and the United Kingdom. In the United States, it spent eight weeks atop the Billboard 200 and was the best-selling album of 2020. Three songs, \"Cardigan\", \"The 1\", and \"Exile\" featuring Bon Iver, reached the top 10 on international singles charts, with \"Cardigan\" peaking at number one on the Billboard Hot 100.\n",
      "Folklore received widespread critical acclaim for its emotional weight and intricate lyricism; some journalists commented that its introspective tone was timely for the pandemic, and they regarded its sound as a bold reinvention of Swift's artistry. Many publications featured the album on their 2020 year-end rankings, and Rolling Stone included it in their 2023 revision of their \"500 Greatest Albums of All Time\" list. Folklore won Album of the Year at the 63rd Annual Grammy Awards, making Swift the first woman to win the award three times. The album informed the concept of Swift's next record, Evermore (2020), boosted Dessner's reputation, and has inspired other artists' works.\n",
      "\n",
      "\n",
      "== Background ==\n",
      "In April 2020, Taylor Swift was set to embark on Lover Fest, a concert tour in support of her seventh studio album Lover (2019), which was cancelled following the COVID-19 pandemic. On July 23, 2020, nine photos were uploaded to Swift's Instagram account, all without captions, forming a black and white image of her standing alone in a forest. Subsequently, Swift made another post across all her social media accounts, announcing that her eighth studio album would be released at midnight; Swift stated: \"Most of the things I had planned this summer didn't end up happening, but there is something I hadn't planned on that DID happen. And that thing is my 8th studio album, Folklore\". She confirmed the image as the album's cover artwork and revealed the track list. The Wall Street Journal opined that the surprise announcement \"caught fans and the music business off-guard\". Billboard stated that it \"blindsided the pop music world\", arriving as \"exciting news\" during lockdown. Folklore was released eleven months after Lover—the fastest turnaround for a Swift studio album at the time, beating the one year and nine months gap between Reputation (2017) and Lover. In another post, Swift announced that the music video for the track \"Cardigan\" would release at the same time as the album.\n",
      "During the YouTube premiere countdown to the \"Cardigan\" music video, Swift hinted that the album lyrics contained many of her signature Easter eggs: \"One thing I did purposely on this album was put the Easter eggs in the lyrics, more than just the videos. I created character arcs and recurring themes that map out who is singing about who... For example, th' metadata={'title': 'Folklore (Taylor Swift album)', 'summary': 'Folklore is the eighth studio album by the American singer-songwriter Taylor Swift. It was surprise-released on July 24, 2020, by Republic Records. Conceived during quarantine in early 2020, amidst the COVID-19 pandemic, the album explores themes of escapism, nostalgia, and romanticism. Swift recorded her vocals in her Los Angeles home studio and worked virtually with the producers Aaron Dessner and Jack Antonoff, who operated from their studios in the Hudson Valley and New York City.  \\nUsing a set of characters and story arcs to depict fictional narratives, the album departs from the autobiographical songwriting that had characterized Swift\\'s past albums. Experimenting with new musical styles, Folklore consists of mellow ballads driven by piano, strings, and muted percussion; music critics classify the genre as a blend of folk, pop, alternative, electronic, and rock subgenres. The album\\'s title was inspired by the lasting legacy of folktales, and its visual aesthetic adopts a cottagecore style.\\nFolklore was accompanied by the concert documentary Folklore: The Long Pond Studio Sessions, featuring Swift\\'s commentary and performances. The album topped the charts in Australasia and various European countries and was certified platinum or higher in Australia, Austria, Denmark, Italy, New Zealand, Norway, Poland, and the United Kingdom. In the United States, it spent eight weeks atop the Billboard 200 and was the best-selling album of 2020. Three songs, \"Cardigan\", \"The 1\", and \"Exile\" featuring Bon Iver, reached the top 10 on international singles charts, with \"Cardigan\" peaking at number one on the Billboard Hot 100.\\nFolklore received widespread critical acclaim for its emotional weight and intricate lyricism; some journalists commented that its introspective tone was timely for the pandemic, and they regarded its sound as a bold reinvention of Swift\\'s artistry. Many publications featured the album on their 2020 year-end rankings, and Rolling Stone included it in their 2023 revision of their \"500 Greatest Albums of All Time\" list. Folklore won Album of the Year at the 63rd Annual Grammy Awards, making Swift the first woman to win the award three times. The album informed the concept of Swift\\'s next record, Evermore (2020), boosted Dessner\\'s reputation, and has inspired other artists\\' works.', 'source': 'https://en.wikipedia.org/wiki/Folklore_(Taylor_Swift_album)'}\n"
     ]
    }
   ],
   "source": [
    "print(wiki_docs[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e224411a",
   "metadata": {},
   "source": [
    "#### Data Transformation Techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b808ca70",
   "metadata": {},
   "source": [
    "Chunking using RecursiveCharacterTextSplitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "170c976f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks created:  748\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap = 20)\n",
    "transformed_pdf = text_splitter.split_documents(pdf_docs)\n",
    "print('Total Chunks created: ', len(transformed_pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ac7ebf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages in the pdf:  231\n",
      "Original Document:  page_content='4 Pre-training\n",
      "Unsupervised Supervised\n",
      "Pre-training Training\n",
      "Unlabeled\n",
      "Data\n",
      "Labeled\n",
      "Data\n",
      "(a) Unsupervised Pre-training\n",
      "Supervised Supervised\n",
      "Pre-training Tuning\n",
      "Labeled\n",
      "Data\n",
      "T ask 1\n",
      "Labeled\n",
      "Data\n",
      "T ask 2\n",
      "(b) Supervised Pre-training\n",
      "Self-\n",
      "Supervised\n",
      "Supervised\n",
      "Zero/Few\n",
      "Shot Learning\n",
      "Pre-training Tuning\n",
      "Prompting\n",
      "Unlabeled\n",
      "Data\n",
      "Labeled\n",
      "Data\n",
      "(c) Self-supervised Pre-training\n",
      "Fig. 1.1: Illustration of unsupervised, supervised, and self-super vised pre-training. In unsupervised pre-training, the\n",
      "pre-training is performed on large-scale unlabeled data. I t can be viewed as a preliminary step to have a good starting\n",
      "point for the subsequent optimization process, though cons iderable effort is still required to further train the model\n",
      "with labeled data after pre-training. In supervised pre-tr aining, the underlying assumption is that different (super vised)\n",
      "learning tasks are related. So we can ﬁrst train the model on o ne task, and transfer the resulting model to another task\n",
      "with some training or tuning effort. In self-supervised pre -training, a model is pre-trained on large-scale unlabeled data\n",
      "via self-supervision. The model can be well trained in this w ay , and we can efﬁciently adapt it to new tasks through\n",
      "ﬁne-tuning or prompting.\n",
      "• Sequence Generation Models . In NLP , sequence generation generally refers to the prob-\n",
      "lem of generating a sequence of tokens based on a given contex t. The term context has\n",
      "different meanings across applications. For example, it re fers to the preceding tokens in\n",
      "language modeling, and refers to the source-language seque nce in machine translation\n",
      "2 .\n",
      "W e need different techniques for applying these models to do wnstream tasks after pre-training.\n",
      "Here we are interested in the following two methods.\n",
      "1.1.2.1 Fine-tuning of Pre-trained Models\n",
      "For sequence encoding pre-training, a common method of adap ting pre-trained models is ﬁne-\n",
      "tuning. Let Encodeθ (·) denote an encoder with parameters θ, for example, Encodeθ (·) can be a\n",
      "standard Transformer encoder. Provided we have pre-traine d this model in some way and obtained\n",
      "the optimal parameters ˆθ, we can employ it to model any sequence and generate the corre sponding\n",
      "representation, like this\n",
      "H = Encode ˆθ (x) (1.2)\n",
      "where x is the input sequence {x0 ,x1 ,...,xm }, and H is the output representation which is a\n",
      "sequence of real-valued vectors {h0 ,h1,...,hm }. Because the encoder does not work as a stan-\n",
      "dalone NLP system, it is often integrated as a component into a bigger system. Consider, for\n",
      "example, a text classiﬁcation problem in which we identify t he polarity (i.e., positive, negative,\n",
      "2 More precisely , in auto-regressive decoding of machine tra nslation, each target-language token is generated based\n",
      "on both its preceding tokens and source-language sequence.' metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'foundation_of_llm_tao_zhu.pdf', 'total_pages': 231, 'page': 10, 'page_label': '4'}\n"
     ]
    }
   ],
   "source": [
    "print('Total pages in the pdf: ', len(pdf_docs))\n",
    "print('Original Document: ', pdf_docs[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5373017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of one chunk: \n",
      "\n",
      "page_content='v\n",
      "2.3.5 Position Extrapolation and Interpolation . . . . . . . . . . . . . . . . . . 82\n",
      "2.3.6 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n",
      "2.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "3 Prompting 96\n",
      "3.1 General Prompt Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n",
      "3.1.1 Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n",
      "3.1.2 In-context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n",
      "3.1.3 Prompt Engineering Strategies . . . . . . . . . . . . . . . . . . . . . . . 101\n",
      "3.1.4 More Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n",
      "3.2 Advanced Prompting Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n",
      "3.2.1 Chain of Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n",
      "3.2.2 Problem Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . 117' metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'foundation_of_llm_tao_zhu.pdf', 'total_pages': 231, 'page': 5, 'page_label': 'v'}\n"
     ]
    }
   ],
   "source": [
    "print('Content of one chunk: \\n')\n",
    "print(transformed_pdf[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8ad5c529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks created:  231\n",
      "173\n"
     ]
    }
   ],
   "source": [
    "char_splitter = CharacterTextSplitter(separator=\"\\n\\n\",chunk_size=100, chunk_overlap= 20)\n",
    "transformed_pdf_charSplit = char_splitter.split_documents(pdf_docs)\n",
    "print('Total number of chunks created: ', len(transformed_pdf_charSplit))\n",
    "print(len(transformed_pdf_charSplit[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488c5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 470, which is longer than the specified 100\n",
      "Created a chunk of size 347, which is longer than the specified 100\n",
      "Created a chunk of size 668, which is longer than the specified 100\n",
      "Created a chunk of size 982, which is longer than the specified 100\n",
      "Created a chunk of size 789, which is longer than the specified 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks created:  7\n"
     ]
    }
   ],
   "source": [
    "loader=TextLoader('speech.txt')\n",
    "docs=loader.load()\n",
    "text_splitter=CharacterTextSplitter(separator=\"\\n\\n\",chunk_size=100,chunk_overlap=20)\n",
    "split_text_docs = text_splitter.split_documents(docs)\n",
    "print('Total number of chunks created: ', len(split_text_docs))\n",
    "print(len(split_text_docs[0].page_content)) # why is the length of the chunk greater than the chunk_size mentioned above i.e. 100 ???? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65ea89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_ai_2_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
